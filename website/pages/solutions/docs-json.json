[{"dirname":"architecture_hexgagonal","id":0,"path":"target/generated-docs/solutions/architecture_hexgagonal/index.html","type":"solution","title":"Hexagonal architecture","body":"\nHexagonal architecture\nThis article explains so-called Hexagonal Architecture which is one of architectural styles applicable for wide variety of applications / software.\nYou have the following problem to be solved\nWe need to develop our software in more robust way so that it can survive technological changes.\nYou should be not forced to rewrite big amount of business logic code just because you need to:\nreplace specific transport layer protocol,\nor replace storage technology,\nor you want to migrate from one cloud provider to another cloud provider,\nor …​\nEventually, you don’t want your domain code base to be scattered and obscured with technological details such as HTTP exceptions, SQL code, JPA/Spring annotations, etc.\nThe proposed solution enables the customer\nWith appropriate architecture that is technology agnostic the customer should expect:\npredictable and limited costs of migration to newer version of technological components (including \"breaking changes\" migrations),\nmanageable costs of replacing technological components (such as exchanging SQL provider or migrating from Springboot to Quarkus),\nmanageable costs of infrastructure shifts (such as changing cloud provider),\nlower overall maintenance costs due to cleaner domain code and higher software quality.\nThe proposed solution is\naddressing your business\nThis solution is most valuable for medium-complexity to high-complexity software systems.\nExpected benefits are measured as further development and maintenance costs as presented on the following pseudo-graph by Martin Fowler (see bibliography for source).\nHexagonal architecture can be applied to any business domain, however it works best with domains that have rich business rules.\ndefinitively needed if\nHexagonal architecture is preferred solution if:\nthere is a high probability of exchanging certain technical component in the future,\nthere is a necessity of applying a DDD technique (Domain Driven Design).\nHow to solve your problem\nBe domain centric\nPlan and implement your domain model without any technical dependencies.\nUse standard features of your programming language as they are sufficient to realize your goals.\nYou can use power of object oriented programming and model entities or aggregates with methods mutating their state.\nBe use case centric\nPlan and implement your use cases without any technical dependencies.\nUse domain model as the only dependency to your use cases.\nThe use case is basically a function that uses and interacts with domain model.\nThe domain and use cases form \"the core\" as shown in the center of following diagram.\nIsolate and protect your core\nKeep your domain model and use cases isolated from technical details.\nAs each use case is basically a function, call this function from glue code called inbound adapter.\nThe interface between your use case and the adapter is called inbound port.\nConvert data from inbound specific form into use case specific form if necessary.\nIf your use case must call any external system (that is usually another API, event queue, database), use inversion of control pattern and hide that external system behind interface.\nWe call such interface an outbound port.\nManage your dependencies\nYour ultimate goal is to keep your system core clean of any technical dependencies.\nIt makes more sense to present such architecture using concentric circles rather than stacked layers.\nWith such visualisation in mind you have to ensure that dependencies always point inwards and never in opposite direction (refer dashed arrows on following diagram).\nIt is important to note, that layers inside the core, as visible in \"onion\" model are optional.\nYou can put as many layers as you want, depending on needs.\nThe bare minimum seems to be however: domain and use cases.\nUse appropriate tooling\nTo practically manage dependencies you can use:\nmultiple source modules (using Maven or Gradle),\nArchUnit,\ncombine both if needed.\nGo beyond!\nRelated documentation\nThe Hexagonal Architecture by Alistair Cockburn\nThe Clean Architecture by Robert C. Martin\nThe Onion Architecture by Jeffrey Palermon\nClean Architecture: A Craftsman’s Guide to Software Structure and Design, Robert C. Martin\nGet your hands dirty on clean architecture, Tom Hombergs\nIs quality worth costs by Martin Fowler\nPractical example by Herberto Graca\nHex Thai Star - reimplementation of My-Thai-Star (WiP)\nReady for changes with Hexagonal Architecture by Damir Svrtan and Sergii Makagon\n"},{"dirname":"cloud_storage_overview","id":1,"path":"target/generated-docs/solutions/cloud_storage_overview/index.html","type":"solution","title":"Cloud Storage","body":"\nCloud Storage\nThis article mainly focuses on below topics:\n- What is Cloud Storage?\n- How does it works?\n- Benefits of Cloud Storage\n- Cloud Storage requirement\n- Types of Cloud Storage\n- Product and services offered by different vendors\nWhat is Cloud Storage?\nCloud Storage is a cloud computing model in which data is transmitted and stored on remote storage systems, where it is maintained, managed, backed up and made available to users over a network — typically, the internet.\nIt’s delivered on demand with just-in-time capacity and costs, and eliminates buying and managing your own data storage infrastructure. This gives you agility, global scale and durability, with “anytime, anywhere” data access.\nHow does it works?\nCloud Storage is purchased from a third party cloud vendor who owns and operates data storage capacity and delivers it over the Internet in a pay-as-you-go model. Typically, you connect to the storage cloud either through the internet or a dedicated private connection, using a web portal, website, or a mobile app. The server with which you connect forwards your data to a pool of servers located in one or more data centers, depending on the size of the cloud provider’s operation.Applications access Cloud Storage through traditional storage protocols or directly via an API.\nThese Cloud Storage vendors manage capacity, security and durability to make data accessible to your applications all around the world.\nCloud Storage is available in private, public and hybrid clouds.\nPublic Cloud Storage: In this model, you connect over the internet to a storage cloud that’s maintained by a cloud provider and used by other companies.\nPrivate Cloud Storage: Private Cloud Storage setups typically replicate the cloud model, but they reside within your network, leveraging a physical server to create instances of virtual servers to increase capacity. You can choose to take full control of an on-premise private cloud or engage a Cloud Storage provider to build a dedicated private cloud that you can access with a private connection.\nHybrid Cloud Storage: This model combines elements of private and public clouds, giving organizations a choice of which data to store in which cloud.\nBenefits of Cloud Storage\nTotal Cost of Ownership:\nWith Cloud Storage, there is no hardware to purchase, storage to provision, or capital being used for \"someday\" scenarios. You can add or remove capacity on demand, quickly change performance and retention characteristics, and only pay for storage that you actually use.\nLess frequently accessed data can even be automatically moved to lower cost tiers in accordance with auditable rules, driving economies of scale.\nTime to Deployment:\nCloud Storage allows IT to quickly deliver the exact amount of storage needed, right when it’s needed. This allows IT to focus on solving complex application problems instead of having to manage storage systems.\nInformation Management:\nCentralizing storage in the cloud creates a tremendous leverage point for new use cases. By using Cloud Storage lifecycle management policies, you can perform powerful information management tasks including automated tiering or locking down data in support of compliance requirements.\nScalability: Growth constraints are one of the most severe limitations of on-premise storage. With Cloud Storage, you can scale up as much as you need. Capacity is virtually unlimited.\nLimitations of Cloud Storage\nSecurity: Security concerns are common with cloud-based services. Cloud Storage providers try to secure their infrastructure with up-to-date technologies and practices,but occasional breaches have occurred, creating discomfort with users. Security is shared responsibility with cloud providers and users.\nLatency: Delays in data transmission to and from the cloud can occur as a result of traffic congestion, especially when you use shared public internet connections.\nRegulatory compilance: Certain industries, such as healthcare and finance, have to comply with strict data privacy and archival regulations, which may prevent companies from using Cloud Storage for certain types of files, such as medical and investment records.\nType of Cloud Storage\nMajorly, below are main types of Cloud Storage:\nObject Storage: It manages data as objects. Each object includes the data in a file, its associated metadata, and an identifier. Objects store data in the format it arrives in and makes it possible to customize metadata in ways that make the data easier to access and analyze. Instead of being organized in files or folder hierarchies, objects are kept in repositories that deliver virtually unlimited scalability. Since there is no filing hierarchy and the metadata is customizable, object storage allows you to optimize storage resources in a cost-effective way.\nFor example, S3(Simple Storage Service) in AWS, Blob Storage in Azure and Google Cloud Storage in GCP are object storage.\nFile Storage: The file storage method saves data in the hierarchical file and folder structure with which most of us are familiar. The data retains its format, whether residing in the storage system or in the client where it originates, and the hierarchy makes it easier and more intuitive to find and retrieve files when needed. File storage is commonly used for development platforms, home directories, and repositories for video, audio, and other files.\nFor example, EFS and FSx are file storage services in AWS, azure file storage in Azure and Google Cloud Filestore in GCP.\nBlock Storage: Block storage, sometimes referred to as block-level storage, is a technology that is used to store data files on Storage Area Networks (SANs) or cloud-based storage environments. Developers favor block storage for computing situations where they require fast, efficient, and reliable data transportation.Block storage breaks up data into blocks and then stores those blocks as separate pieces, each with a unique identifier. The SAN places those blocks of data wherever it is most efficient.\nBlock storage also decouples data from user environments, allowing that data to be spread across multiple environments. This creates multiple paths to the data and allows the user to retrieve it quickly.\nFor example, EBS in AWS and Google Cloud Persistent Disks in GCP.\nAlso, archival and database services can be considered for data storage\nProducts &amp; Services\nBelow table contains services from different cloud vendors:\nVendor\nStorage Services\nDatabase Services\nBackup Services\nAWS\n• Simple Storage Service (S3)\n• Elastic Block Storage (EBS)\n• Elastic File System (EFS)\n• Storage Gateway\n• Snowball\n• Snowball Edge\n• Snowmobile\n• Aurora\n• RDS\n• DynamoDB\n• ElastiCache\n• Redshift\n• Neptune\nGlacier\nAzure\n• Blob Storage\n• Queue Storage\n• File Storage\n• Disk Storage\n• Data Lake Store\n• SQL Database\n• Database for MySQL\n• Database for PostgreSQL\n• Data Warehouse\n• Server Stretch Database\n• Cosmos DB\n• Table Storage\n• Redis Cache\n• Data Factory\n• Archive Storage\n• Backup\n• Site Recovery\nGCP\n• Cloud Storage\n• Persistent Disk\n• Transfer Appliance\n• Transfer Service\n• Cloud SQL\n• Cloud Bigtable\n• Cloud Spanner\n• Cloud Datastore\nNone\nReferences:\nhttps://www.ibm.com/cloud/learn/cloud-storage#toc-what-is-cl-vt64lltQ\nhttps://aws.amazon.com/what-is-cloud-storage/\n"},{"dirname":"communication","id":2,"path":"target/generated-docs/solutions/communication/index.html","type":"solution","title":"Communication among services","body":"\nCommunication among services\nCommunication is a very important aspect while building IT application, especially large and complex one or one with microservices architecture. Based on your analysis of your problems, considering the following basic questions to start with:\nWhich services have to communicate with which services? to exchange which type of data?\nHow should the communication look like? in a synchronous way or asynchronous way?\nOnce you have your answers, make your choices on concrete protocols, products and libraries. Using multiple of them in your application is fine.\nUnfortunately, there is no silver bullet. Each communication type has its own advantages and disadvantages and target a different scenario and goals. With that said, do not always apply REST or always apply message-driven approaches (e.g. Kafka) just because you have heard some success stories about it.\nHere we try to expose cross-cutting best practices for communication among services in larger IT application landscapes on a high level. Details how to implement this with specific libraries or programming-languages are described in the individual stacks of devonfw.\nCommunication types\nGenerally, types of communication can be classified in two axes.\nThe first axis defines if the protocol is synchronous or asynchronous:\nSynchronous protocol: The client sends a request and waits for a response from the service as the client code can only continue its task when it receives the server response. That’s independent of the client code execution that could be synchronous (thread is blocked) or asynchronous (thread isn’t blocked, and the response will reach a callback eventually). e.g. HTTP/HTTPs\nAsynchronous protocol: The client code or message sender usually doesn’t wait for a response. It just sends the message as when sending a message to a queue or any other message broker. e.g AMQP\nThe second axis defines if the communication has a single receiver or multiple receivers:\nSingle receiver: Each request must be processed by exactly one receiver or service.\nMultiple receivers: Each request can be processed by zero to multiple receivers. This type of communication must be asynchronous. An example is the publish/subscribe mechanism.\nOne common style is single-receiver communication with a synchronous protocol like HTTP/HTTPS when invoking a regular Web API HTTP service. Microservices also typically use messaging protocols for asynchronous communication between microservices. We will discuss more about some popular approaches adopted these days including:\n[rpc]\n[messaging_and_eventing]\n[service_mesh]\nRPC\nWhen it comes to RPC communication nowadays we immediately talk about HTTPS.\nThe most common choice is REST with JSON that perfectly works together with web browsers.\nHowever, please also consider other options especially for backend communication:\ngRPC is an efficient, high performance RPC protocol\n…​\nProtocols like SOAP or even RMI, Corba, etc. should be considered as legacy and discouraged.\nMessaging and eventing\nWe do not even try to distinguish between messaging and eventing.\nA message can be seen as an event and an event as a message.\nThe nature of this communication style is that it is entirely asynchronous and the sender of the event or message does not have to know about the receipient(s).\nThere can even be many recipients for the same event or message.\nTypically there is a central messaging system (event bus) that is all you need to know about to send and receive your events or messages.\nThis leads to a very loose coupling of your services.\nWhile this adds flexibility it also can add quite some complexity to understand what is actually going on.\nDebugging and tracing the communication can get really hard.\nIn the worst case you can lose control and end up with cyclic events triggering each other till eternity.\nHowever, when properly applied, this communication style can make your architecture very powerful and extendable.\nOne of the most common choices for such a messaging system is kafka. Other message brokers to be considered are RabbitMQ and ActiveMQ. In an enterprise environment where often a Oracle Database is in place you might also consider Oracle AQ.\nService mesh\nIn the context of microservices, service mesh is a key to communication.\nService-to-service communication is what makes microservices possible.\nBut creating, routing and managing this communication, both within and across application clusters, becomes increasingly complex as the number of services grows.\nService mesh solves this problem. It takes the logic governing service-to-service communication out of individual services and abstracts it to a layer of infrastructure.\nIt also captures many aspects of the communication as security (e.g. encryption) or performance metrics which can be used to monitor the cluster as well as identify and analyze system failures if occurs.\nOne of the most common and best choices for service mesh is istio.\nIstio is an open-source service mesh with powerful features that enable developers to easily configure\nthe traffic flow\nthe authentication and authorization among internal and to external services and\nmonitoring tools such as kiali, prometheus and grafana.\nCommunication contracts\nA communication contract is a structured document that defines the expected input and output of a service.\nThere are two approaches to using contracts for communications:\nContract first: There is a design document that defines the formal contract of the communication and the code artefacts are derived with some tooling.\nCode first: Code is written directly and documentation is generated from there.\nWhich approach is better depends on your project setup.\nWhile code first often feels more lightweight and brings less over head, for a more complex project setup the formality of the contract could be very beneficial.\nContracts can be used both for synchronous or asynchronous communications, for example Open Api has become the standard for REST contracts and Async Api is growing in adoption for the asynchronous needs.\nLinks\nREST vs messaging for microservices\nEventing and messaging\nKafka microservices\nCommunication in a microservices architecture\nGetting started with istio\nKeycloak and istio\nImplementation hints\nREST:\nJava Server\nJava Client\nAngular\n.NET/C#\nnode.js\nKafka:\nJava\n"},{"dirname":"endusermgmt_azure_aad","id":3,"path":"target/generated-docs/solutions/endusermgmt_azure_aad/index.html","type":"solution","title":"Enduser Management Solutions - Azure Active Directory","body":"\nTable of Contents\nEnduser Management Solutions - Azure Active Directory\nEnduser Management Introduction\nContext &amp; Problem\nStandard Problems\nEnduser Management Platforms\nAzure\nEnduser Management Solutions - Azure Active Directory\nOverview Azure Active Directory\nEnduser Management Application Level\nEnduser Management Infrastructure Level\nCredits\nEnduser Management Solutions - Azure Active Directory\nEnduser Management Introduction\nContext &amp; Problem\nThis document describes patterns to manage the endusers of applications including authentication and authorization. Applications are assumed to be web applications. From a data modeling perspective the major entities are the application and its endusers.\nApplication scenarios can range from relatively simple (Just the users with a few additional data) to very complex:\nSimple: User and an application with a few extra attributes.\nMiddle: Users with different scopes. Global users are independent from the app whereas app specific users are targeted to a certain application only (See here for details).\nVery complex\nOnline marketing (also known as internet marketing or web marketing) includes all marketing activities that are carried out online to achieve marketing goals, from brand awareness to closing an online business. Factors that make it complex:\nState change: Various events can trigger state changes in such a scenario. Targeting users with marketing content usually requires consent by the user. Expiry or explicit withdrawal must be reflected in those systems.\nBusiness partners as endusers: Business partners have to go through a much more complicated process before being verified. This might also include backoffice activities that trigger further state changes. Leaving the business or closing of business triggers state changes in the associated users.\nGuest user: To maximize online-business users should also allowed to do business although they may not yet fully registered in the system.\nAPI: Systems should have a clear defined API to shield calling applications from implementation details and allowing changing of underlying platform. An API also simplifies versioning.\nIntegration: Certain aspects such as authentication might split across multiple systems. Systems have to communicate in such a scenario and might have to be able to receive signals to move forward in the processing flow.\nData modeling: Complex scenarios have a much more complex data model which might include arbitrary attributes and own custom entities.\nTechnically enduser management can be integrated into the application itself or factored out into a separate service. The pictures below indicate the basic options:\nInside Application: In this model the persistence layer also hosts the additional enduser data. All required functionality such as authentication/ authorization is provided by the application.\nFactored out (Same platform as app): In that case the enduser management is factored out in a separate service within the same platform as the application.\nFactored out (Different platform as app): In that case the enduser management is factored out in a separate service using a platform, that differs from the application platform.\nHybrid: Mixture of options 1 to 3.\nAs shown enduser management can be split across systems. For the components used the standard considerations from the software lifecycle apply. This includes operational aspects such as monitoring and provisioning. Especially testing is challenging since the functionality requires user interaction which is not available in automated tests.\nThe picture below shows a logical architecture of a complex scenario:\nStandard Problems\nThe following standard problems will be addressed in subsequent paragraphs. The problems result from all described application scenarios:\nEntire process enduser management\nThe folowing challenges exist regarding the entire process:\nImplementation (Testing): Testing is a challenge since the tested functionality requires user interaction.\nImplementation (Provisioning): Automatic deployment pipelines should be preferred. The challenge might rise from the fact that the developers reside in a different organizational boundary than the endusers of the application.\nOperations (Monitoring): The user interactions must be monitored end-to-end across enduser management and the pplication. This might be challenging if enduser management is factored out in a separate service.\nApplication\nThe application must be able to access enduser information regarding authentication/ authorization.\nThe application must manage sign in, out and up. Changes in the enduser management must be reflected by the application also if the user has not initiated a certain action.\nEnduser managment\nA service must able to handle the following challenges:\nAPI: A clear definition of the interfaces would preferable. This also allows to address versioning and abstraction of the underlying enduser management implementation.\nFlows: Flows refer to a sequence of actions mainly related to authorization and authentication. The flows are highly asyncronous due to possible blocking enduser interaction. E.g. if the user has to confirm his identity with an EMail verification link.\nExternal: Bidrectional communication is required if functionality is split across systems. E.g. if the backoffice approval is factored out in a different systems that requires adjusting the state of the enduser.\nCustom logic: It must be possible to incorporate custom logic e.g. to get additional data such as loyalty id.\nData store: Endusers state must be persisted along with all asscociated information.\nScheduling: Consent by the enduser might be limited to a certain expiry date. Expiry usually requires state changes of the enduser in the enduser management service.\nOn behalf of: Especially if business partners as endusers are involved it is common that others do things on behalf of others. E.g. if the owner (=enduser) of a business partner organization invites employees.\nAccess Reviews: Endusers you have been granted access might not be there anymore. E.g. because they have left their home organization. In that case system support is helpful that allows to determine which users are still active.\nEnduser Management Platforms\nAzure\nThis chapter lists major features/ concrete services for enduser management of the Azure platform. This architecture pattern builds on the general problem description for enduser management. A detailed discussion of services is part of the solution design in the subsequent chapters. Major features for the previously described areas are as follows:\nEnduser managment\nThe central control instance for all users within Azure is Azure Active Directory (=AAD). Users in AAD must exist in a so called tenant which constitutes the highest possible domain boundary within Azure.\nAAD comes with features that support enduser management out of the box such as AAD B2C. These features cover quite well the simple to middle scenario. In the complex scenario limitations of these features might be a problem. In that case only Azure native services remain as fallback. Problematic service limitations might be:\nOwn entities: The features only support custom attributes but no own entities.\nSeparate tenant: AAD B2C comes with an own tenant that differs from the corporate one where the provisioning services might live. Azure DevOps pipelines don’t work if you use custom policies.\nBidirectional communication: Receiving messages in a publish/ subscribe pattern might not be directly supported.\nAPIs: AAD comes with its defined endpoints. It is not possible to introduce custom APIs.\nMultiple organizations: Especially when endusers can be business partners users might be associated with multiple organizations. A tenant can serve as organization. However, a tenant is a heavyweight construct and moving endusers between tenants is not as easy as a data store update.\nAAD B2C provides cookie based session management with different options to control the session. Upon subsequent authentication requests, Azure AD B2C reads and validates the cookie-based session, and issues an access token without prompting the user to sign in again. If the cookie-based session expires or becomes invalid, the user is prompted to sign-in again. Sessions can be managed by the following parties:\nAzure AD B2C - Session managed by Azure AD B2C\nFederated identity provider - Session managed by the identity provider, for example Facebook, Salesforce, or Microsoft account\nApplication - Session managed by the web, mobile, or single page application\nApplication\nMicrosoft provides AAD libraries that wrap the required functionalities including sign in/ ou/ up.\nEntire process enduser management\nThe Azure platform supports enduser management as follows:\nImplementation (Testing): User flows can be tested separatly without being incorporated into an application. No additional support beyond that feature exists.\nImplementation (Provisioning): Automatic deployment pipelines should be preferred. The challenge might rise from the fact that the developers reside in a different organizational boundary than the endusers of the application.\nOperations (Monitoring): Azure provides Azure Monitor for monitoring the enduser management service from the application perspective. Monitoring the AAD service from infrastructure perspective is more difficult.\nThe picture below summarizes major aspects mentioned before:\nEnduser Management Solutions - Azure Active Directory\nOverview Azure Active Directory\nThe central control instance for all users within Azure is Azure Active Directory (AAD). It comes with the following features/ services that can be used for enduser management:\nAAD Business to Customer (B2C)\nThis feature started with the idea to cover endusers of an application. The feature comes with a special B2C tenant in which the users are located.\nAAD Business to Business (B2B)\nThis feature started with the idea to give external users access to Azure resources in an existing tenant. The external are modeled as guest users in AAD. This feature can be also used for modeling endusers especially if the same tenant is not problematic. A big advantage is that problems related to the separate B2C tenant are not relevant such as provisioning.\nCurrently there is a lot of movement in this area. Microsoft was already porting features from B2C to B2B. The new external identities marks the shift towards even more integration between the two. Currently it refers to two things: (1) the new umbrella term unifying the so far separate B2B/ B2C features and (2) additional features that were added to AAD B2B. According to this source this might mean a unified service offering that combines the B2B and B2C features.\nAccording to feedback from the microsoft product group it probably does not mean a a new separate service that unifies the existing B2B and B2C. The majority is voting for creatin an option within AAD B2B to group the users in a logical container within a tenant. The current hassle could be then avoided that comes with the different B2C one.\nEnduser Management Application Level\nOverview\nThe solution is to use one of the introduced AAD features and the following platform features. The focus of this chapter is to introduce the relevant features. Recommendations for a concrete setup are given in the next chapter.\nThe relevant AAD features are as follows:\nAAD Application registration\nApplications need to be registered for usage within Azure B2C and B2B. Registration is mandatory for Azure B2C. B2B session settings define whether a user has access to certain applications only or a wider scope such as single-sign-on for all app registrations in the B2C tenant. For B2B it is only mandatory if users are invited per application (Scopes user group and tenant don’t require an app registration). App registration contain settings per Applications such as redirect URLs. See here for an example of an SPA web application.\nAAD User Management feature\nAAD B2B and B2C can be possible to manage end users. Crucial questions are:\nIs it a problem to store the application users with others in the same tenant? (Since you might have a mixture of corporate and non corporate users)\nDoes the separate tenant cause problems? (E.g. because you want to use custom policies which results causes problems regarding provisioning)\nFeature differences such as Single Sign On\nMonitoring\nApplication monitoring can be done via Azure Monitor. However the required steps for setting it up differ from standard. It requires even additional services such as Azure Lighthouse due to the separate tenant.\nCompliance\nAzure User features can be combined with various other Azure services/ AAD features to strengthen security. This includes:\nAzure sentinel for security analytics based on routing/ audit logs forwarded\nAD B2C Identity Protection provides two reports. The Risky users report is where administrators can find which users are at risk and details about detections. The risk detections report provides information about each risk detection, including type, other risks triggered at the same time, sign-in attempt location and more.\nThe picture below summarizes the points:\nDetailed AAD User Management Features\nThe AAD features support the following out of the box:\nApplication\nMicrosoft provides AAD libraries that wrap the required functionalities including sign in/ ou/ up.\nEnduser managment\nThe AAD supports the following:\nFlows: Supported out of the box. HTML/ CSS customization of required pages is possible. Flows are defined declarativ via XML.\nCustom logic: Can be implemented via functions.\nData store: Supported entities besides users and organizations (=tenants) can be found here. Custom attributes can be added but no own entities.\nOn behalf of: Is provided out of the box by invitation workflows.\nAccess Reviews: Is provided out of the box.\nVariations\nAuthentication can be done by an external identity provider.\nEnduser Management Infrastructure Level\nThe core service is Azure AD. A direct monitoring with Azure Services is hard since the infrastructure monitoring tools also require Azure AD authentication. Possible native tools are:\nMicrosoft 365 service health status page\nMicrosoft 365 admin center\nThe Microsoft 365 Service Health Dashboard (SHD) doesn’t send notifications about Azure AD service outages and only shows this information in a dashboard, which administrators have to pull up and analyze (link to Source).\nCredits\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:14 UTC\n"},{"dirname":"iot_edge_processing_ms","id":4,"path":"target/generated-docs/solutions/iot_edge_processing_ms/index.html","type":"solution","title":"IoT Edge Data Processing","body":"\nTable of Contents\nIoT Edge Data Processing\nIoT Edge Data Processing Introduction\nContext &amp; Problems\nIoT Platforms\nStandard Problems\nIoT Edge Data Processing Platforms\nMicrosoft\nIoT Edge Data Processing Solutions\nMicrosoft\nThird Party Container Offerings\nIoT Edge Data Processing\nIoT Edge Data Processing Introduction\nContext &amp; Problems\nThe solutions focus is at the data processing part at the edge within IoT platforms.\nIoT Platforms\nThe Internet of things (IoT) describes physical objects (or groups of such objects) with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other local communications networks. The location of the devices is also known as edge. Industrial IoT (IIoT) is a subcategory of IoT. IIoT utilizes the principles of the internet of things in industrial settings or, in short, “the elements are identical, but the usage varies.” IIoT is often used about Industry 4.0, a term that encapsulates the manufacturing industry’s current trend to utilize a combination of IoT, Big Data, cloud, and edge computing.\nBroadly conceived, an IoT platform is a multi-layer technology that facilitates the provisioning and management of connected devices within the IoT ecosystem consisting of IoT endpoints, edge hardware, networks, and a broader value chain. IoT platforms are often referred to as middleware solutions as they act as the connective tissue between the data collected from edge devices and the end-user applications. According to Gartner: “An IoT platform is an on-premises software suite or a cloud service…that monitors and may manage and control various types of endpoints, often via applications business units deploy on the platform.” The broader value chain consists of the following:\nSensors, actuators, controllers gathering data\nA communication network\nAn IoT edge device/gateway that aggregates the data and transmits it across the network\nData processing platforms e.g. for transforming, modeling, and visualizing data\nEnd-user applications\nSome IoT platforms are less comprehensive than others. Depending on your Applications, you may require an IoT platform that only covers a certain portion of the value chain or needs a complete solution. An end-to-end IoT platform that covers the full IoT development cycle will typically include the following (https://www.iotforall.com/iiot-iot-or-mes-platforms-whats-the-difference):\nIoT device management\nData collection capabilities\nIoT data modeling, analytics, visualization\nApp development capabilities or a marketplace for IoT apps\nIoT collaboration enablement, social features\nAn IIoT platform is special case of an IoT platform with a wider scope of offered features, the specific industry focus, and the emphasis on industry-level security standards.\nTo implementing IoT platforms two major approaches can be distinguished:\nTraditional approach used so far\nMES, or manufacturing execution systems, have been the traditional way to streamline operations all the way down to the shop floor. They manage industrial assets, collect data, and ensure traceability in an industrial setting, among others. As a more traditional approach to IIoT, MES are centralized solutions. They tend to encounter difficulties when having to perform outside of their main functionalities. MES tend to be vendor-specific and proprietary, so industrial manufacturers can only work with their vendor’s modules. And you can buy the vendor’s apps only. The manufacturers have little control over the applications they develop and deploy. (Source: https://www.iotforall.com/iiot-iot-or-mes-platforms-whats-the-difference)\nnewer (I)IoT platforms\nIIoT platforms are more flexible as they tend to be microservice-based and decentralized. They usually consist of communication software for monitoring, troubleshooting, and managing the connected IoT devices, the network, and data flows. On top of that, some IIoT platforms come with advanced analytics capabilities and support for IoT app development.\nThe picture below summarizes the major actors of a full blown decentralized (I)IoT Platform:\nStandard Problems\nThe described problems are split in following categories:\nFunctional problems\nFocus is the compute capacity that is connected to the device and the cloud.\nNon-Functional\nRefers to standard non-functional problems such as Monitoring, Provisioning, Business Continuity, Scaling or Security.\nThe list below describes the standard functional problems in the above scenarios:\nConnectivity (Device ⇐⇒ Edge Processing)\nThis might be challenging for various reasons:\nDevices might not support standard protocols such as OPC-UA/ MQTT/ AMQP ⇒ might require protocol translation\nDevices might require small footprint of installed software ⇒ requires protocols that don’t need big software packages on device side\nDevice might not support protocols with encryption\nConnection might be shaky\nTypical protocols in that area are:\nHTTPS as fallback if nothing else works\nMQTT especially for shaky connections and has a smaller footprint compared to AMQP\nAMQP has a larger footprint but extra features like connection multiplexing\nMQTT/ AMQP use special ports that are normally blocked by firewalls. A common solution is to run MQTT/ AMQP over WebSockets.\nConnectivity (Edge Processing ⇐⇒ Cloud)\nCloud connectivity might be limited so that caching solutions are required.\nProcessing\nProcessing must be able to handle the incoming frequency of data. If data cannot be processed and forwarded at the incoming speed an overflow of storage might happen.\nDevice Data Normalization/ Aggregation\nData received from the devices might differ in various aspects:\nThe same logical value such as 100°C might be encoded by a different data type or by a different value\nThe interval by which devices send data might differ\nData received from devices is a snapshot at a certain point in time. For processing an average for a certain time window needs to be calculated.\nData enrichment\nData received from the devices might need to be enriched with additional metadata. A classic example is the corresponding customer for a rented device such as a container. The sensor only sends a technical identifier and measured values but not the current owner of the container.\nThe list below describes the standard non-functional problems in the above scenarios:\nMonitoring: Major options are between monitoring on-premises within the edge or remotely via cloud. The concrete platform depends heavily on the processing platform.\nProvisioning: The concrete platform depends heavily on the processing platform.\nScaling: Scaling mechanisms of the processing platform apply if that feature exists out of the box. Otherwise, manual scaling must be implemented if the processing platform is not node aware.\nSecurity\nSecurity in transit, at rest and in extreme cases during processing must be ensured. If the device cannot send the data encrypted the transmission must be hardened by additional measures.\nIoT Edge Data Processing Platforms\nMicrosoft\nOverview\nThe subsequent chapters give an overview about infrastructure services/ components from Microsoft and Third Party to host processing logic within an IoT platform at the edge. Recommendations are given to allow a solution architect to choose an option.\nOfferings Categories\nVarious Microsoft offerings use Azure Arc as enabling technology for on-premises/ edge. Azure Arc allows it to move operations of edge resources (VM, Kubernetes based) in the cloud. This includes monitoring and governance by applying security policies. Key component is an agent on the edge resources that establishes a bidirectional communication channel to the cloud. Edge resources appear then in the Azure control plane as cloud hosted service instances. The picture below shows the major idea:\nThe presented offerings fall in the following categories:\nPrivate Azure datacenter: The offering is intended if you create your own on-premises datacenter that is running Azure cloud software and certified hardware from Microsoft.\nCluster of nodes\nPluggable Device: The device is plugged into the on-premises host to extend its functionality.\nIoT specialized Operating System (=Single Server)\nContainer based platforms (Software only)\nThe major options are:\nNon node aware modular platform ⇒ This can be combined with Kubernetes for node awareness and scaling\nBare metal Kubernetes platforms which can be further split into lightweight platforms especially targeted for IoT or standard Kubernetes options\nIoT Edge Data Processing Solutions\nMicrosoft\nThe Microsoft offerings are as follows:\nService\nCategory\nArchitecture\nSupported OS\nLimitations\nAdditional Functionality\nCloud Integration\nCosts\nAzure stack HUB\nCluster (Own Datacenter)\nWindows/ Linux (Detailed Versions)\nOnly integrated OEM HW + Azure Microsoft\nMinimum 4-16 servers for one scale unit\nAzure general services like Azure Resource Manager + Subset of Azure Services\nConnected/ disconnected\nAzure Marketplace connection\nYes (via IoT Edge Gateway in Stack Hub)\nPay-as-you go as in Azure (same subscriptions, Azure Prepayments, and billing tools as Azure)\nDiscount for reserved instances not possible\nAzure Stack Edge\nPluggable device (Intended for AI inference) due to powerful hardware\nWindows/ Linux (Detailed Versions)\nSoftware requirements hosts (OS, protocols device access, storage accounts)\nNetworking requirements device (Port, protocol)\nKubernetes\nAzure Arc Integration possible\nYes (via registration in IoT Hub)\nmonthly subscription fee for using Azure Stack Edge for a period of thirty (30) calendar days\nShipping and customs will be charged extra\nNo upfront cost\nAzure Stack HCI\nCluster (Not entire datacenter)\nWindows/ Linux (Detailed Version)\nMinimum HW: Single node\nValidated HW from catalog (with over 200 entries)\nBought either HW + SW or SW only\nStorage Spaces (Software Defined Storage)\nAzure Arc enabled cloud operations incl. Backup, Monitoring, Compliance, deployment target as custom location\nYes (via registration in IoT Hub)\nUsage fee on a per-core basis on your on-premises serve\nAzure subscription\nAdditional fees for guests\nAKS on Azure Stack HCI\nCluster (Not entire datacenter)\nHost can be windows due to windows server\nKubernetes nodes Linux per default\nWindows container require windows nodes\nMany additional system requirements on top of HCI (AD, HW, networking, Windows Admin Center, Azure)\nAKS\nyes\nUsage per vCPU of running worker nodes within workload clusters (If hyperthreading activated only half of the costs)\nBilling data sent to Azure\nAzure IoT Edge\nContainer based platform\nWindows container only to windows hosts\nLinux containers to windows and Linux hosts\nSupported OS host\nAssumes Moby-engine as container platform\nMinimum assumption regarding host since Linux/ Windows virtualization platform (e.g. works in Hyper-V, vSphere, IoT Device)\nIntegration with IoT Hub\nModul systems with built-in communication\nModules available via Marketplace (Functions, Azure Stream Analytics)\nYes\nFee per device for Azure Stream Analytics\nCosts for IoT Hub (IoT Hub used for device mgmt and services deployed to edge)\nWindows IoT\nOperating system\nWindows\nIoT Core limited windows 10 support (e.g. only Universal Windows Platform)\nRuns on any hardware with the \"Certified for Windows Server\" logo. See the&nbsp;WindowsServerCatalog\nDevice Mgmt for supported devices\nYes (integration possible)\nIoT Core: Online Licensing Agreement and Embedded OEM Agreements, Royalty-free\nIoT Enterprise: Direct and Indirect Embedded OEM Agreements\nThe following recommendations exist:\nNo built-in functionality required such as Azure Arc, device mgmt &amp; IoT Hub, specialized HW, caching or other showstoppers (Know-how, Budget) ,\nNone of the presented options  Whatever you like\nHardware constraints new/ existing violated or software only\nAzure IoT Edge especially when modular systems + IoT Hub. This option is not node aware. To combine this option on Kubernetes Microsoft recommends Virtual Kubes (https://docs.microsoft.com/en-us/azure/iot-edge/how-to-install-iot-edge-kubernetes?view=iotedge-2020-11).\nWindows 10 IoT if modular needed\nManual Azure Arc installation if required\nNo HW constraints violations\nHCI biggest package (Arc, Kubernetes)\nSpecial focus AI/ powerful HW  Azure Stack Edge\nEntire Datacenter  Azure Stack Hub\nThird Party Container Offerings\nBare metal platforms lightweight Kubernetes platforms that might be also installed on the device itself are:\nMiniKube: Downvoted since for testing purposes only (https://www.itprotoday.com/cloud-computing-and-edge-computing/lightweight-kubernetes-showdown-minikube-vs-k3s-vs-microk8s)\nMicroK8s: Conformant \"Low Ops\" Kubernetes by Canonical (HA support with multiple master nodes has been added recently)\nK3s: Certified Kubernetes distribution built for IoT and edge computing\nKubeEdge\nIt assumes control plane in cloud and worker nodes can also be at the edge. KubEdge can be installed on the device itself (See here for source https://groups.google.com/g/kubeedge/c/VBhKAJSqmDc). The architecture is shown below (Red boxes mark KubeEdge components):\nBare metal platforms standard compute:\nKubeAdm\nKubernetes tool for creating ground-up Kubernetes clusters which is good for standard compute (Linux/Windows) (https://medium.com/@sven_50828/setting-up-a-high-availability-kubernetes-cluster-with-multiple-masters-31eec45701a2). KubeAdm supports multi master Kubernetes clusters. A Kubernetes cluster with a broken single master cannot be administrated anymore e.g., to add new nodes. Running pods will still continue to run.\nThe following decision tree can be used. \"Low Ops\" in the tree refers to the decreased cost of operations when some operational tasks are abstracted or made easier, like auto updates or simplified upgrades:\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:18 UTC\n"},{"dirname":"microservices_azure_aks","id":5,"path":"target/generated-docs/solutions/microservices_azure_aks/index.html","type":"solution","title":"Microservice Platforms Solutions - Azure Kubernetes Service","body":"\nTable of Contents\nMicroservice Platforms Solutions - Azure Kubernetes Service\nMicroservice Platforms Introduction\nContext &amp; Problem\nIntroduction to Kubernetes\nStandard Problems\nMicroservice Platforms\nAzure\nMicroservice Platforms Solutions - Azure Kubernetes Service\nInfrastructure\nApplication\nCredits\nMicroservice Platforms Solutions - Azure Kubernetes Service\nMicroservice Platforms Introduction\nContext &amp; Problem\nMicroservice orchestration is the automatic process of managing or scheduling the work of individual microservices of an application within a cluster. The platform provides an automated process of managing, scaling, and maintaining microservices of an application.\nThe container orchestration platform Kubernetes is the current de facto standard (Docker swarm and others are nowadays neglectable compared to Kubernetes). Containers are units that packages up code and all its dependencies like libraries so that the application can be started quickly and runs reliably, regardless of the infrastructure.\nContainer orchestration tools automate the management of various tasks that software teams encounter in a container’s lifecycle, including the following: Container deployment, scaling, load balancing and traffic routing, network and container configuration, allocation of resources, gathering of insights, provisioning, scheduling, distribution of containers to physical hosts, service discovery, health monitoring, cluster management (Link).\nIntroduction to Kubernetes\nTo achieve the above goals a rough understanding of the Kubernetes structure is important.\nThe control plane is responsible for managing your container workload. All action in Kubernetes goes through the api-server which receives and executes commands. The workload is defined by the target state you define in so called objects. A Kubernetes object is a \"record of intent\" - once you create the object, the Kubernetes system will constantly work to ensure that object exists. Those definitions can exist in manifest files, or be obtained from the api-server.\nPods are the smallest, most basic deployable objects in Kubernetes. A Pod represents a single instance of a running process in your cluster. Pods contain one or more containers. When a Pod runs multiple containers, the containers are managed as a single entity and share the Pod’s resources. However, to make life considerably easier, you don’t need to manage each Pod directly. Instead, you can use workload resources that manage a set of pods on your behalf such as a \"deployment\". These resources configure controllers that make sure the right number of the right kind of pod are running, to match the state you specified (See here for concepts implementing workflows).\nTwo major extensions can be distinguished. There is a certain overlap between both with both having a different focus. Service meshes focus at core container orchestration functionality. The other class are extensions that focus at supporting typical application patterns such as publish subscribe. Infrastructure from programming perspective falls into two categories:\nPlatform orchestration support: Provided by Kubernetes and service meshes\nApplication platform support: Provided by additional tools such as DAPR\nThe picture below summarizes the major aspects:\nStandard Problems\nThis chapter focus at problems that are unique due to the microservice focus. Links to documentation of microservice agnostic parts are given below. An example for a problem with microservice specifics and agnostic parts is provisioning. Creating the orchestration infrastructure is conceptually not different from other resources that are created with a pipeline. However, the additional container build step is specific to microservices based on a container orchestration platform.\nThe following standard problems regarding the orchestration platform support will be addressed in subsequent paragraphs:\nDeployment\nOn orchestration platform level special options exist how to create various environments. For general aspects of provisioning see the pattern \"Provisioning\".\nScaling\nTargets of scaling can be the underlying nodes (=VMs) of the orchestration cluster and the pods per node. Scaling can be manual by stating a target number of components or automatic e.g. depending on load.\nLoad balancing/traffic routing\nThis is core strategy for maximizing availability and scalability, load balancing distributes network traffic among multiple backend services efficiently. A range of options for load balancing external traffic to pods exists in the Kubernetes context, each with its own benefits and tradeoffs. Basic options are:\nLoad Balancing with kube-proxy: Simple but not fair if clients send with different frequency\nKubernetes Endpoints API: Load balancer uses Kubernetes API to track availability of pods\nIngress Load Balancer: Most popular and allows for sophisticated load balancing rules\nThe above list only shows the major hooks/ solutions that are available in Kubernetes. More variations are possible if basic functions are taken into account:\nBackend discovery\nHealth Checking\nDistribution algorithm such as round robin\nProtocol level e.g. OSI layer 7 or layer 4 only\nNetworking\nNetworking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. There are 4 distinct networking problems to address:\nTightly coupled container-to-container communications: this is solved by ** Pods and localhost communications.\nPod-to-Pod communications: this is the primary focus of this document.\nPod-to-Service communications: this is covered by services.\nExternal-to-Service communications: this is covered by services.\nKubernetes uses the following model to organize networking. Every Pod gets its own IP address. This means you do not need to explicitly create links between Pods and you almost never need to deal with mapping container ports to host ports. Pods on a node can communicate with all pods on all nodes without NAT. Kubernetes IP addresses exist at the Pod scope - containers within a Pod share their network namespaces - including their IP address and MAC address. This means that containers within a Pod can all reach each other’s ports on localhost.\nConfiguration\nConfiguration has various dimensions:\nSensitive versus non-sensitive information\nOrchestration platform versus application settings\nAutomatic deployment of configuration settings versus manual\nScheduling\nIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of terminating one or more Pods on Nodes.\nFactors that need to be taken into account for scheduling decisions include individual and collective resource requirements, hardware / software / policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and so on.\nService discovery\nService discovery is the actual process of figuring out how to connect to a service (Link). The approach can be either (1) client or (2) server driven.\nIn case of client-side discovery the client is responsible for determining which service instance it should connect to. It does that by contacting a service registry component, which keeps records of all the running services and their endpoints. When a new service gets added or another one dies, the Service Registry is automatically updated. It is the client’s responsibility to load-balance and distribute its request load on the available services.\nIn the server-side discovery a load-balancing layer exists in front of the service instances. The client connects to the well-defined URL of the load balancer and the latter determines which backend service it shall route the request too. Because a Pod can be moved or rescheduled to another Node, any internal IPs that this Pod is assigned can change over time. If we were to connect to this Pod to access our application, it would not work on the next re-deployment. To make a Pod reachable to external networks or clusters without relying on any internal IPs, we need another layer of abstraction. Services provide network connectivity to Pods that work uniformly across clusters. Each service exposes an IP address, and may also expose a DNS endpoint — both of which will never change. Internal or external consumers that need to communicate with a set of pods will use the service’s IP address, or its more generally known DNS endpoint. In this way, the service acts as the glue for connecting pods with other pods.\nApplication services\nStandard services on application level include:\nService-to-Service invocation\nState management\nPublish &amp; Subscribe: This pattern allows microservices to communicate with each other using messages. The producer or publisher sends messages to a topic without knowledge of what application will receive them. This involves writing them to an input channel. Similarly, a consumer or subscriber subscribes to the topic and receive its messages without any knowledge of what service produced these messages. This involves receiving messages from an output channel. An intermediary message broker is responsible for copying each message from an input channel to an output channels for all subscribers interested in that message. This pattern is especially useful when you need to decouple microservices from one another.\nResource &amp; Binding Triggers: Using bindings, you can trigger your app with events coming in from external systems, or interface with external systems.\nSecrets\nThe following standard problems regarding the applications to be deployed will be addressed in subsequent paragraphs:\nDesigning\nMutiple options exist how many containers an application consists of. In the simplest case persistence is achieved by persistent volumes but also hosting entire databases on Kubernetes is possible.\nProvisioning\nContainer images need to be built, stored in a registry and deployed. Additional challenges during built are for instance triggering dependent images if a base image is affected or enforcing quality gates such as security scans as part of the build pipelines. Deployments might even go beyond Kubernetes if you have for instance a rolling update with database changes.\nCompliance\nCompliance affects the building of containers and the running application such as restricting communication between containers.\nConfiguration\nContainers need to be configured. An additional challenge might therefore to inject environment specific values.\nMonitoring\nYou can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster. Kubernetes provides detailed information about an application’s resource usage at each of these levels. This information allows you to evaluate your application’s performance.\nMicroservice Platforms\nAzure\nThis chapter lists major features/ concrete services for microservices platforms within Azure. A detailed discussion of services is part of the solution design based on a certain service.\nAzure provides the following container platforms:\nAzure Kubernetes Service (AKS)\nIt represents a hosted Kubernetes service. Additional features on orchestration platform include the integration with other Azure services such as Azure Active Directory concepts.\nAzure Container Instance (ACI)\nThis service is intended to run single containers without native orchestration support. However, it can be integrated into Kubernetes.\nAzure Red Hat OpenShift (ARO)\nIt provides highly available, fully managed OpenShift clusters as orchestration platform on demand, monitored and operated jointly by Microsoft and Red Hat. Kubernetes is at the core of Red Hat OpenShift. OpenShift brings added-value features to complement Kubernetes, making it a turnkey container platform as a service (PaaS) with a significantly improved developer and operator experience.\nAzure Container Apps (in preview as of 07.11.2021)\nAzure Container Apps allows to build serverless microservices based on containers. Distinctive features of Container Apps include:\nOptimized for running general purpose containers, especially for applications that span many microservices deployed in containers.\nPowered by Kubernetes and open-source technologies like Dapr, KEDA, and envoy.\nSupports Kubernetes-style apps and microservices with features like service discovery and traffic splitting.\nEnables event-driven application architectures by supporting scale based on traffic and pulling from event sources like queues, including scale to zero.\nSupport of long running processes and can run background tasks.\nAll Container Apps are Kubernetes compatible.\nAzure Container Apps doesn’t provide direct access to the underlying Kubernetes APIs. If you require access to the Kubernetes APIs and control plane, you should use Azure Kubernetes Service. However, if you would like to build Kubernetes-style applications and don’t require direct access to all the native Kubernetes APIs and cluster management, Container Apps provides a fully managed experience based on best-practices.\nAzure Spring Cloud\nAzure Spring Cloud makes it easy to deploy Spring Boot microservice applications to Azure without any code changes. The service manages the infrastructure of Spring Cloud applications so developers can focus on their code. Azure Spring Cloud provides lifecycle management using comprehensive monitoring and diagnostics, configuration management, service discovery, CI/CD integration, blue-green deployments, and more. If your team or organization is predominantly Spring, Azure Spring Cloud is an ideal option.\nRegarding the applications to be deployed Azure comes also with its own Azure container reistry (ACR).\nThe picture below summarizes major points:\nMicroservice Platforms Solutions - Azure Kubernetes Service\nInfrastructure\nOverview\nThe solution is to use Azure Kubernetes Service and the following platform features regarding. The focus of this chapter is to introduce the relevant features. Recommendations for a concrete setup are given in the next chapter.\nThe platform features that (can) complement Kubernetes are:\nThe services that (can) complement Kubernetes:\nAdvisory\nProactive and actionable recommendations from Azure Advisor based on your configuration and usage telemetry as described here.\nProvisioning\nUse Bridge to Kubernetes to iteratively develop, test, and debug microservices targeted for AKS clusters. It is a client-only experience offered through extensions in Visual Studio and Visual Studio Code. See also Provisioning for general aspects and service options for creating pipelines for creating infrastructure.\nCompliance\nUse security measures on networking level to avoid public IPs. Combine AKS with additional services to control ingress and outgoing traffic such as Application Gateway or firewalls.\nEnforce compliance rules to your cluster and CI/CD pipeline consistently with Azure Policy. Azure Active Directory provides access control with role-based-access-controls (RBAC) and service principals/ managed identities to back RBAC roles. Integration with Azure Security Center can provide security management, intelligent threat detection and actionable recommendations.\nDesaster recovery\nHigher availability using redundancies across availability zones, protecting applications from datacenter failures. Paired region deployment for disaster recovery.\nMonitoring\nFor infrastructure monitoring see \"Monitoring\". For infrastructure monitoring specific pages in Azure Monitor exist which will be described here.\nThe picture below summarizes some of the services mentioned above:\nsolution_microservices_azure_aks_infra_detailed_native_setup\nApplication\nOverview\nThe solution is to deploy the containerized application to an Azure Kubernetes Service. Focus of that chapter are designing, building, monitoring and deploying containerized applications. Recommendations for a concrete setup are given in the next chapter.\nThe services that (can) complement Kubernetes:\nDesigning\nEach Pod is meant to run a single instance of a given application. If you want to scale your application horizontally (to provide more overall resources by running more instances), you should use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as replication. Replicated Pods are usually created and managed as a group by a workload resource and its controller.\nThe \"one-container-per-Pod\" model is the most common Kubernetes use case. A more advanced use case is running multiple containers in a pod that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit of service - for example, one container serving data stored in a shared volume to the public, while a separate sidecar container refreshes or updates those files. The Pod wraps these containers, storage resources, and an ephemeral network identity together as a single unit.\nContainers have to store information persistently. Azure provides Azure (managed) disks and Azure files as storage options for persistent volumes.\nAKS can connect with databases via wrapper objects such as Services or databases can be directly deployed to Kubernetes. Options for deplyong a database directly to Kubernetes are given below:\nSQL server (Microsoft): Options range from single sql server to high availability with failover groups. In both cases MS provides containers that contain sql server.\nThird party options such as PostgreSQL\nConfiguration\nConfigmaps are useful to store non-critical data in key-value pair format. They can also be used to inject env vars into pods. Secrets are useful to store sensitive data in key value pair format. They can also be used to inject env vars into pods. You can optionally specify how much of each resource a container needs. The most common resources to specify are CPU and memory (RAM).\nCompliance\nA security context defines privilege and access control settings for a pod. Examples are:\nDiscretionary Access Control: Permission to access an object, like a file, is based on user ID (UID) and group ID (GID).\nSecurity Enhanced Linux (SELinux): Objects are assigned security labels.\nRunning as privileged or unprivileged.\nLinux Capabilities: Give a process some privileges, but not all the privileges of the root user.\nAppArmor: Use program profiles to restrict the capabilities of individual programs.\nAllowPrivilegeEscalation: Controls whether a process can gain more privileges than its parent process.\nreadOnlyRootFilesystem: Mounts the container’s root filesystem as read-only.\nDisks used in your AKS cluster can by encrypted by using your own keys through Azure Key Vault.\nSee building for additional security measures when containers are built.\nBuilding (CI part of provisioning)\nBuilding containers includes the following steps:\nBuilding the container image(s)\nPushing the image(s) to the registry\nProvisioning tools such as Azure DevOps and Gitub Actions provide special docker tasks/ activities to build images. Pushing to registries is also supported. The following additional features can be used/ should be considered from security perspective:\nEach time a base image is updated, you should also update any downstream container images. Integrate this build process into validation and deployment pipelines such as Azure Pipelines or Jenkins. These pipelines make sure that your applications continue to run on the updated based images. Once your application container images are validated, the AKS deployments can then be updated to run the latest, secure images. Azure Container Registry Tasks can also automatically update container images when the base image is updated.\nA container security scan can be included in the pipelines as quality gate by using tools like tools such as Twistlock or Aqua.\nThe provisioning services support Docker Content Trust (DCT). Docker Content Trust (DCT) are digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags.\nDeployment (CI part of provisioning)\nThe term \"Deployment\" refers to the process that triggers a deployment in Kubernetes whereas a Kubernetes deployment refers to the Kubernets deployment resource. A kubernetes deployment resource is the standard controller for manipulating pods which in turn host the container workloads.\nA deployment is triggered by the provisioning pipeline. Depending on the scope a deployment goes beyond the a kubernetes deployment that results in a Kubernetes deployment resource. The various steps across various scenarios can be generalized as follows:\nPre-Kubernetes Deployment steps\nKubernetes Deployment\nAzure provisioning services provide ways to trigger with native kubernetes means such as manifests by supporting special tasks/ activties. However, this results in quite a number of files you have to maintain. Additional tools like helm (see variation) provide better support.\nPost-Kubernetes Deployment steps\nThe most complex deployment scenario is a rolling update with breaking database changes. In that case pre and post Kubernetes deployment steps are required to handle the breaking database changes. Such an update requires targeting specific components e.g. with a certain version. Labels are key/value pairs that are attached to objects, such as pods. They help in filtering out specific objects. Using a Selector, the client/user can identify a set of objects. Annotations are used to attach arbitrary non-identifying metadata to objects.\nThe basic idea is to break down the breaking database change into multiple non-breaking steps. The steps below refer to a renaming of a column:\nAdd a db migration that inserts the new column\nChange the app so that all writes go to the old and new column\nRun a task that copies all values from the old to the new column\nChange the app that it reads from the new column\nAdd a migration that remove the old column\nMonitoring\nApplication logs can help in understanding the activities and status of the application. The logs are particularly useful for debugging problems and monitoring cluster activity. Monitoring applications can be done by storing logs and studying the application’s metrics.\nTools like Prometheus-Grafana are popular as they make the management of metrics very easy. Very often, sidecar containers are used as metrics exporters of the main application container.\nBy integrating with Azure Monitor, a Prometheus server is not required. You just need to expose the Prometheus metrics endpoint through your exporters or pods (application), and the containerized agent for Container insights can scrape the metrics for you.\nVariations\nThe following additional extra tools can be used in conjunction with Kubernetes:\nDeployment\nInstead of having to write separate YAML files for each application manually, you can simply create a Helm chart and let Helm deploy the application to the cluster for you. Helm charts contain templates for various Kubernetes resources that combine to form an application.\nA Helm chart can be customized when deploying it on different Kubernetes clusters. Helm charts can be created in such a way that environment or deployment-specific configurations can be extracted out to a separate file so that these values can be specified when the Helm chart is deployed. The snippet below shows a template using placeholders to refer to the values in values.yaml:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: {{ .Values.postgres.name }}\nlabels:\napp: {{ .Values.postgres.name }}\ngroup: {{ .Values.postgres.group }}\nspec:\nreplicas: {{ .Values.replicaCount }}\nselector:\nmatchLabels:\napp: {{ .Values.postgres.name }}\n...\nCompliance\nFor security reasons and improvement of Helm charts, it is useful to make use of at least one Helm linting tool to ensure your deployments are valid and versioned correctly.\nWhy choosing Polaris as Linting Tool: For helm chart linting, there are several tools like Polaris, kube-score or config-lint available. With Polaris, checks and rules are already given by default, whereby other tools need a lot of custom rules configuration and are therefore more complex to setup. Polaris runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping to avoid problems in the future. Polaris can be either installed inside a cluster or as a command-line tool to analyze Kubernetes manifests statically.\nConfiguration\nSee under infrastructure.\nWhen to use\nWhen you want to deploy containerized applications to Azure Kubernetes Service.\nCredits\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:21 UTC\n"},{"dirname":"monitoring_aws_cloudwatchCustomAlarm","id":6,"path":"target/generated-docs/solutions/monitoring_aws_cloudwatchCustomAlarm/index.html","type":"solution","title":"Customise your cloudwatch alerts","body":"\nTable of Contents\nCustomise your cloudwatch alerts\nYou have the following problem to be solved\nThe proposed solution enables the customer\nRelated Architectures and Alternatives\nProducts &amp; Services\nCustomise your cloudwatch alerts\nYou have the following problem to be solved\nCloudwatch alarms give you the ability to notify you in case an alarm is triggered. The standard message layout of cloudwatch is very inflexible and not customisable. With this approach it is possible to fill HTML templates with further information, links and buttons to find the solution ASAP.\nThe proposed solution enables the customer\nto send out an customised e-mail template which looks more professional and has additional information compared to the standard cloudwatch alarm.\nRelated Architectures and Alternatives\nSee code here: https://github.com/AlessandroVol23/cloudwatch-custom-email-cdk\nProducts &amp; Services\nCloudwatch Alarm: Some cloudwatch alarm\nSNS Topic which will be triggered by the cloudwatch alarm\nLambda which will be called by the SNS topic\nSES will be used by the lambda to send out HTML emails\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:23 UTC\n"},{"dirname":"monitoring_azure_native","id":7,"path":"target/generated-docs/solutions/monitoring_azure_native/index.html","type":"solution","title":"Monitoring Solutions - Azure Native","body":"\nTable of Contents\nMonitoring Solutions - Azure Native\nMonitoring Introduction\nContext &amp; Problem\nMonitoring Platforms\nAzure\nMonitoring Solutions - Azure Native\nApplication Monitoring\nInfrastructure Monitoring\nCredits\nMonitoring Solutions - Azure Native\nMonitoring Introduction\nContext &amp; Problem\nTerminology\nUsed definition: A monitoring solution helps the monitoring consumer achieve the satisfactory level of control of a defined service. (Link to source)\nThis definition already includes the following:\nDefined service: The resources you want to monitor aka monitored resources. The resources to be monitored can be split in infrastructure and applications on top.\nLevel of control: That is your bandwidth in which your defined service operates normally aka known as baseline\nMeasuring: A measurement is a single act that quantifies an attribute of a part, equipment, service or process (CPU load, available memory etc.). Data measured is emitted by the monitored resources and aka telemetry.\nMonitoring consumer: The user trying to keep the service within its baseline boundaries.\nThe key to achieve that is a single control plane is usually preferred to simplify the operations for the consumer aka monitoring plane. The relevant content depends on the perspective of the consumer such as performance, costs, compliance and health. Performance in this pattern includes the following as described here:\nHealth monitoring: purpose of health monitoring is to generate a snapshot of the current health of the system so that you can verify that all components of the system are functioning as expected.\nError monitoring: Bugs &amp; errors need to be detected by monitoring. Supporting information must be provided that allows monitoring consumer to analyze the root cause.\nAvailability monitoring: A truly healthy system requires that the components and subsystems that compose the system are available. Availability monitoring is closely related to health monitoring. But whereas health monitoring provides an immediate view of the current health of the system, availability monitoring is concerned with tracking the availability of the system and its components to generate statistics about the uptime of the system.\nPerformance monitoring: As the system is placed under more and more stress (by increasing the volume of users), the size of the datasets that these users access grows and the possiblity of failure of one or more components becomes more likely. Frequently, component failure is preceded by a decrease in performance. If you’re able detect such a decrease, you can take proactive steps to remedy the situation.\nSLA Monitoring: SLA monitoring is closely related to performance monitoring. But whereas performance monitoring is concerned with ensuring that the system functions optimally, SLA monitoring is governed by a contractual obligation that defines what optimally actually means. You can calculate the percentage availability of a service over a period of time by using the following formula: %Availability = ((Total Time – Total Downtime) / Total Time ) * 100\nProviding a control plane requires a monitoring pipeline that should be implemented as feedback loop. The pipeline transforms raw telemetry into meaningful information that the monitoring consumer can use to determine the state of the system. The loop ensures that lessons learnt are the starting point for further improvements on the defined service side. E.g. by adaptive scaling depending on monitored traffic. The entire monitoring must be compliant and provide integration features. The conceptual stages of the pipeline are as follows:\nData Sources/ Instrumention (Monitored resources): concerned with identifying the sources from where the telemetry needs to be captured, determining which data to capture and how to capture it.\nCollection/ Storage (Monitoring plane)\nAnalysis/ Diagnosis (Monitoring plane): generate meaningful information that an monitoring consumer can use to determine the state of the system\nVisualization/ Alerting (Monitoring plane): decisions about possible actions to take and then feed the results back into the instrumentation and collection stages\nThe picture below summarizes the aspects:\nStandard Problems\nThe list below describes the standard problems that apply independent from the monitoring consumer perspective. Solutions with concrete technology are first described in subsequent chapters. Per monitoring pipeline stage the following standard problems are known:\nData Sources/ Instrumention (Monitored Resources)\nThis also includes the possibility of preprocessing to reduce or enrich sent telemtry data to the monitoring consumer. Telemetry itself might be of different structure and convey different information.\nCollection/ Storage (Monitoring Plane)\nThe drop location of the telemetry needs to be determined such as inside the monitoring plane or externally.\nMonitoring can result in a large amount of data. Storing such granular data is costly. Therefore an archiving mechanism is required to make sure costs are not exploding. Once archived the ingested telemetry should be removed.\nAnalysis/ Diagnosis (Monitoring Plane)\nIncludes standard problems like:\nFiltering\nAggregation\nCorrelation\nReformating\nComparison against Key Performance Indicatorss (=KPIs). KPIs have no weight in software development unless they are paired with your business goals. You don’t need a handful of KPI metrics for your software team. All you need is the right KPI to help you improve your product or process. KPIs should be SMART (S = Specific; M = Measureable; A = Assignable; R = Realistic; T = Time Bound). Examples: Code Quality KPIs such as Maintainability index, Complexity metrics, Depth of inheritance, Class coupling, Lines of code; Testing Quality such as Test effort, Test coverage; Availability = Mean time between failures, Mean time to recovery/ repair as described here\nVisualization/ Alerting (Monitoring Plane)\nIncludes standard problems like:\nVisualization for monitoring consumer\nAlerts: Programmatic action that free the monitoring consumer from manual intervention. It states the trigger and the action to bee executed. One challenging aspect is to minimize the number of alerts or to detect patterns behind multiple alerts. Infering a suitable thresholds can be challing especially if the threshold is not static.\nReports\nAd-hoc queries\nExploration\nImproving Feedback Loop (Plane/ Resources)\nCases where the monitored resources operated outside their baseline should be the starting point for improvements. This might mean a better tuning of alerts and intervention or system requirements.\nIntegrating and compliance affect the entire pipeline. Telemetry might have to be collected from other systems to achieve a single monitoring plane. However alerts/ notications might have to be forwarded to other systems.\nOf course a monitoring must be compliant regarding the enterprise guidelines.\nThe following patterns are not dicussed here:\nProvisioning of the monitoring plane and the monitored resourves\nFor solutions with a certain technology see the specific guides on platform and concrete service level.\nMonitoring Platforms\nAzure\nOverview\nThis chapter lists major features/ concrete services for monitoring of the Azure platform. This architecture pattern builds on the general problem description for monitoring. The picture below summarizes major services and concepts that are discussed in detail in the next chpater.\nMonitoring Pipeline\nMajor features per stage of the monitoring pipeline are as follows:\nData Sources/ Instrumention\nTelemetry in Azure is split in logs and metrics. Logs contain non-structured text entries whereas metric is a value measured at a certain time. Dimensions are additional characterisitics of the measured metric.\nThe major logs/ metrics are one of the following categories: (1) Activity logs, (2) resource logs (former diagnostic logs) and (3) Azure Active Directory (=AAD) related logs. Activity logs track actions on Azure Resource Manager level such as creation, update or deletion of Azure resources. Resource logs track operations within a resource such as reading secrets from a key vault after it has been created.\nMonitoring Plane\nThe services used for processing depend on the perspective. A major stop for a unified end-to-end monitoring is Azure Monitor. It unifies the former separate services Application Insights and Log Analytics as features. Application Insights is focusing at application monitoring whereas Log Analytics started as part of the operation management suite targeting infrastructure monitoring. Both come with their own repository for storing the telemetry. In the future a Log Analytic Workspace will be the central place for collecting data from infrastructure and application perspective.\nTelemetry can either be (1) forwarded (=pushed) to the monitoring plane or (2) pulled from the monitoring plane.\nPushing can be necessary if the telemetry is not available in Azure monitor out of the box or pulling from the monitored resources is not possible. Monitored resources have to be instrumented to forward telemetry to the monitoring consumer for later processing within the monitoring plane. App insight requires linking via instruentation keys. Log Analytic workspaces require diagnostic settings. Possible targets are only log analytics workspace, event hub or azure blob storage. Telemetry that can be forwarded is predefined. Fine granular selection of metrics/ logs is not always possible.\nPulling reads telemetry such as metrics directly from the monitored resource. Logs cannot be read directly and require pushing. Compared to pushing this method is also faster.\nBoth features cover health and performance perspectives. Cost management is covered by Azure Cost Management. The major services for monitoring compliance are Azure Security Center and Azure Sentinel (Larger enterprise scope compared to Azure Security Center with SIEM and SOAR capabilities).\nAzure monitor provides various options for visualizations but also other services are possible. Dashboards like features provide a single pane of control across a number resources. Kusto is the major language for analyzing logs and metrics e.g. as part of the root cause analysis. Additional features of app insights/ log analytics complement the language.\nAlert thresholds can be dynamic and actions can be grouped in action groups for multiple reuse. Dynamic Thresholds continuously learns the data of the metric series and tries to model it using a set of algorithms and methods as described here. Alerts can be grouped dynamically to reduce noise and filtered/ scoped to reduce false alarms.\nVarious options for archiving exist in Azure such as Logic Apps. A cheap archive is usually Azure blob storage. Policies can be used to automatically delete archived blobs. Removal of ingested telemetry is configurable by setting the retention period accordingly in Log Analytics/ App Insights.\nImproving Feedback Loop (Plane/ Resources)\nThe platform allows to track track end-user behavior and engagement. Impact Analysis helps to prioritize which areas to focus on to improve the most important KPIs as described here. Autoscaling is provided by Azure monitor and other Azure services directly.\nAzure monitor can integrate with and forward telemetry from various sources. Some services like Azure Security center forward telemtry to Azure monitor.\nIT service management tools such as ServiceNow or System Center Service Manager can integrate with Azure monitoring tools.\nAzure provides the standard compliance mechanisms also for monitoring which ensure authentication/ authorization (via Azure Active Directory), compliance for data at-rest and in-transit.\nMonitoring Solutions - Azure Native\nApplication Monitoring\nOverview\nThe solution is to use Azure Monitor with App Insights, Log Analytics and the following platform features regarding the monitored resources. The focus of this chapter is to introduce the relevant features. Recommendations for a concrete setup are given in the next chapter.\nThe relevant Azure monitor features are as follows:\nCollection/ Storage (Monitoring Plane)\nTelemetry can either be stored internally inside the monitoring plane by App Insights/ Log Analytics or externally.\nTelemetry can be pulled from the monitoring plane. This is limited to metrics but faster than pushing. Pushing can be necessary if the telemetry is not available in Azure monitor out of the box or pulling from the monitored resources is not possible. The major mechanisms to push telemetry to the monitoring plane are:\nDiagnostic setting\nApp Insights Instrumentation/ Linking: Linked App Insights must be specified for the monitored resource. Some Azure Services such as Azure App Service come with a built-in App Insight integration. However, other services only provide diagnostic settings instead such as API management.\nManual forwarding: E.g. by scheduled process using the APIs provided by Azure Monitor for App Insights and Log Analytics. A lightweight Azure service for polling services to be monitored is Azure Automation. It allows to host and running scripts.\nApp Insights/ log analytics also provide APIs to manual forward data. However this APIs have some constraints:\ntimestamp cannot be set freely (Both)\ndeleting something is not possible (Both)\nsaved query cannot be updated (App insights only)\nAnalysis/ Diagnosis (Monitoring Plane)\nAzure Monitor comes with no built-in support for KPIs such as code quality, test coverage or availability/ maintenance. However, standard KPIs such as mean time between failure (=MTBF) can be programmed with Kusto queries.\nAzure Application Insights sends web requests to your application at regular intervals from points around the world. It can alert you if your application isn’t responding, or if it responds too slowly as described here.\nKusto queries across multiple application insights or log analytic workspaces are possible. App insight or log analytic workspaces must then be referenced with an additional identifier (App Insights: app('&lt;identifier&gt;'); Log Analytics: (workspace('&lt;qualifier&gt;')) as shown in the samples below. Various options for identifiers exist such as name and guid as described here:\n// Cross-Kusto app insights example\nunion app('mmsportal-prod').requests, app('AI-Prototype/Fabrikam/fabrikamprod').requests, requests\n| summarize count() by bin(timestamp, 1h)\n// Cross-Kusto log analytics example\nunion Update, workspace(\"b438b4f6-912a-46d5-9cb1-b44069212ab4\").Update\n| where TimeGenerated &gt;= ago(1h)\n| where UpdateState == \"Needed\"\n| summarize dcount(Computer) by Classification\nAzure Data Explorer is a service for large scale analysis of telemetry. Large refers large amount of data or high frequency of time series data as described here.\nVisualization/ Alerting (Monitoring Plane)\nNatively Azure monitor provides as dashboarding options (1) Azure dashboards and (2) Azure workbooks.\nAlerts come with the following features:\nTrigger: Results from Kusto queries can be used as trigger.\nAction Groups: Assigning same action (=Action Group) to different triggers\nSmart Groups (Preview as of 24.08.2021): Groups alerts that are triggered simultanously by using artificial intelligence as described here\nAction Rules (Preview as of 24.08.2021): Allows to suppress (e.g. due to maintenance), scope and filter alerts as described here\nReporting: Existing report for SLA/ Outages by using predefined Azure Monitor workbooks from gallery as described here\nApplication Insight comes with the following tools for exploration and root cause analysis:\nApplication Map ⇒ application dependencies in other services such as backend APIs or databases\nSmart Detection ⇒ warn you when anomalies in performance or utilization patterns\nUsage Analysis ⇒ features of your application are most frequently used\nRelease annotations ⇒ visual indicators in your Application Insights charts of new builds and other events. Possible to correlate changes in application performance to code releases.\nCross-component transaction diagnostics ⇒ The unified diagnostics experience automatically correlates server-side telemetry from across all your Application Insights monitored components into a single view. It doesn’t matter if you have multiple resources with separate instrumentation keys. Application Insights detects the underlying relationship and allows you to easily diagnose the application component, dependency, or exception that caused a transaction slowdown or failure as described here.\nSnapshot Debugger ⇒ collect a snapshot of a live application in case of an exception, to analyze it at a later stage.\nCorrelation ⇒ Special fields are provided to convey global identifiers appearing in every request as described here.\nAzure Monitor has also extensive integration features. This includes:\nIntegrating telemetry from other Azure services (e.g. Azure Security Center also forwards to Azure Monitor)\nIntegrating external data sources (e.g. Blobs by using Kusto external operator)\nIntegrating third party tools such as Prometheus for Azure Kuberenetes\nExposing telemtry as data sources for external third party (e.g. Log Analytics Workspaces for Grafana) as described here\nThe following picture summarizes potential Azure services/ features that might be potentially relevant:\nVariations\nA detailed configuration is not possible because the setup depends on the resources to be monitored and their capabilities. Therefore only guidelines are given to infer the right setup:\nCollection/ Storage (Monitoring Plane)\nTwo main decision must be made: (1) storage of telemetry and (2) push versus pull.\nThe number of app insights/ log analytic workspaces needs to be determined per environment. Production should be kept separate already for compliance/ resilience reasons. Dev/ test environments are rather a question mark. Subsuming dev/ test environments into a single monitoring plane is benefecial for the monitoring consumer, since he then has to check only a single place. That also means you need an additional mechanism inferring the environment for later drill down or root cause analysis. Additional custom attributes are recommended if possible. Separate App Insights/ Log Analytic instances per environment require another one for a consolidated dev/ test view.\nMicrosoft recommends a single app insights resource in the following cases as described here:\nFor application components that are deployed together. Usually developed by a single team, managed by the same set of DevOps/ITOps users.\nIf it makes sense to aggregate Key Performance Indicators (KPIs) such as response durations, failure rates in dashboard etc., across all of them by default (you can choose to segment by role name in the Metrics Explorer experience).\nIf there is no need to manage Azure role-based access control (Azure RBAC) differently between the application components.\nIf you don’t need metrics alert criteria that are different between the components.\nIf you do not need to manage continuous exports differently between the components.\nIf you do not need to manage billing/quotas differently between the components.\nIf it is okay to have an API key have the same access to data from all components. And 10 API keys are sufficient for the needs across all of them.\nIf it is okay to have the same smart detection and work item integration settings across all roles.\nStoring telemetry within the monitoring plane is easy to set up if the Azure service supports diagnostic settings or comes with app insights integration. App insights instrumentation allows extensive customization such as preprocessing. Log Analytics allows less customization out-of-the box.\nLog analytics can target cheap Azure blob storage. It can be accessed with Kusto and would also eliminate the need for archiving. However, an shared access signature is required in this case which has to be renewed. Updating a saved query is only possible for Log Analytics workspace. Due to simpler setup storing the telemetry inside the monitoring plane is the recommended option.\nPull via metrics explorer is only possible for metrics but not logs. Pushing via a custom script makes sense if:\nAPI restrictions on monitoring plane are not a problem. E.g. not being able to set the timestamp according to original occurence.\nTracking of UI driven actions that are not pushed automatically\nService targets log analytic workspace but built-in limitations like filtering/ aggregations needed before ingestions in workspace\nThe table below compares various options:\nDiagnostic Settings\nApp Insights Logging\nPush via resource API\nMetrics Explorer\nPossible per resource\n(X)\n(X)\nX\n(X)\nTelemetry Customization\nLimited\nHigh\nLimited-High\nLimited\nCustom Logging in executed code\nX\nTelemetry always captured\nX\n(X)\nX\nX\nLatency\nMedium\nMedium\nMedium\nLow\nDirection\nPush\nPush\nPush\nPull\nComments:\nOption “Push via resource API” ⇒ A scheduled script that reads periodically telemetry and pushes it to monitoring plane using the Rest API\n„Telemetry always captured“ ⇒ Some resources allow multiple ways to run something e.g. via UI or programmatically. If the telemetry is always captured the way does not matter.\nVisualization/ Alerting (Monitoring Plane)\nVarious options inside Azure and by external tools such as Grafana exist. If you are using Grafana you have to (1) find a hosting option and (2) have to connect Grafana with Azure.\nThe basic options are either using the (1) Grafana cloud or (2) hosting Grafana in Azure.\nThe hosting options within Azure can be further diveded into configurations where a single VM with Grafana preinstalled is enough or more sophisticated high availability configurations with additional redundancy on node/ VM level.Hosting options with additional redundancy include:\nInstalling Grafana on multiple nodes as described here via Bitnami Grafana Multi-Tier offering through the Azure Marketplace (Recommended way).\nInstalling Grafana on managed apache cluster instance as described here\nInstalling Grafana on Kubernetes Cluster as described here\nFor connecting Grafana with data source in Azure the options below exist. This also means that Grafana cannot directly connect to Azure Services. Therefore it is required to collect Azure telemetry in Azure places such as Azure Monitor/ Data Explorer:\nAzure Monitor via plugin as described here\nAzure Data Explorer via plugin as described here\nGrafana and Azure Monitor provide visualization and alerts. The following recommenations are intended to help to choose:\nService Management Tool Integration (ITSM): Both can be integrated. See here how to integrate Grafana Events into the ITSM tool ServiceNow. For Azure Monitor connectors exist that depend on the ServiceNow version and arepartially in preview.\nAzure Monitor telemetry is also available via other means (Portal etc.)\nCloud agnostic: Using Grafana opens a cloud agnostic way and could also be used for other clouds\nIncluding critical features in Grafana such as alert might require Grafana hosting with additional redundancy\nGrafana can be integrated with Azure AD or LDAP as described here for authentication\nGrafana is moving to the new version 8.0 which is in public preview (as of 4.11.2021). Machine learning mechanisms e.g. for dynamic thresholds are only in place for Grafana Cloud users which is possible in Azure. A final decision depends on the priorities e.g. cloud agnostic/ drilldown vs. dynamic thresholds. The table below summarizes the features related to the previously introduced Azure dashbaord. See the options below for dashboarding/ visualization:\nAzure\nThird party\nWorkbooks\nDashboards\nPower BI\nGrafana\nAuto refresh in 5 Min Intervall\nX\nX\nX\nFull screen\nX\nX\nX\nTabs\nX\nX\nX\nFixed Parameter lists\nX\nX\nX\nDrill down\nX\nX\nAdditional hosting required\nX\nTerraform Support\nX\nX\nX\nRegarding components for logs/ metrics:\nMetrics: Pull (Metrics explorer) or push (Kusto query targeting data source) possible\nLogs: Push to monitoring plane only\nWhen to use\nThis solution assumes that your application monitoring plane is in Azure and that your monitored resources are located in Azure.\nInfrastructure Monitoring\nOverview\nThe solution is to use Azure Monitor with Log Analytics and the following platform regarding the monitored resources. The focus of this chapter is to introduce the available features. Recommendations for a concrete setup are given in the next chapter.\nThe relevant Azure monitor features are as follows:\nData Sources/ Instrumention\nA major source for infrastructure is the health information provided by the platform. The following health information is relevant:\nService Health Information which also includes planned downtime of the Azure platform and problems on service type level such as VMs\nResource Health which includes health information for service instances you created\nOn resource level resource utilization is relevant. This includes:\nHitting capacity limits regarding CPU/ memory\nIdle resources\nAvailability differs per service. They are usually exposed via metrics.\nCollection/ Storage (Monitoring Plane)\nTelemetry can either be stored internally inside the monitoring plane or externally.\nTelemetry can be pulled from the monitoring plane. This is limited to metrics but faster than pushing. Pushing can be necessary if the telemetry is not available in Azure monitor out of the box or pulling from the monitored resources is not possible. Pushing can be done as follows:\nResource diagnostic: Useful to push resource specific telemtry.\nHealth diagnostic: Resource Health tracks the health of your resources for specific known issues. With diagnostic settings configured on subscription level you can send that data to Log Analytics workspace. You will need to send the ResourceHealth/ Service Health categories (Source Health-Overall Source Possible-Categories).\nAnalysis/ Diagnosis (Monitoring Plane)\nHealth relevant KPIs can be determined via Kusto as shown in the example below:\nAzureActivity\n// Filter only on resource health data in activity log\n| where CategoryValue == 'ResourceHealth'\n// dump any resource health data where the health issue was resolved. We are interested only on unhealthy data\n| where ActivityStatusValue &lt;&gt; \"Resolved\"\n// Column Properties has nested columns which we are parsing as json\n| extend p = parse_json(Properties)\n// Column the parsed Properties column is now a dynamic in column p\n// We take the top level properties of column p and place them in their own columns that start with prefix Properties_\n| evaluate bag_unpack(p, 'Properties_')\n// We do the same for the newly created column Properties_eventProperties\n| extend ep = parse_json(Properties_eventProperties)\n| evaluate bag_unpack(ep, 'EventProperties_' )\n// We list the unique values for column EventProperties_cause\n| distinct EventProperties_cause\nAvailability of resource utilization specific KPIs depends on the monitored resources.\nKusto queries across multiple application insights or log analytic workspaces are possible (See app monitoring for details).\nLog Analytics comes with the following tools for exploration and root cause analysis:\nTable based access allows you to define different permissions per log table. This is done using custom roles where you define the tables as part of the resource type as described here.\nAdditional management solutions: They have to be installed per werkspace. An example is the ITSM Connector used to automatically create incidents or work items when Alerts are created within Log Analytics. Such as System Center Service Manager or Service Now.\nLog analytics agent managentment: agent collects telemetry from Windows and Linux virtual machines in any cloud, on-premises machines, and those monitored by System Center Operations Manager and sends it collected data to your Log Analytics workspace in Azure Monitor. The Log Analytics agent also supports insights and other services in Azure Monitor such as VM insights, Azure Security Center, and Azure Automation as described here.\nService Map automatically discovers application components on Windows and Linux systems and maps the communication between services. Service Map shows connections between servers, processes, inbound and outbound connection latency, and ports across any TCP-connected architecture, with no configuration required other than the installation of an agent as described here.\nVisualization/ Alerting (Monitoring Plane)\nSee Application monitoring features for alerts and visualization.\nThe following picture summarizes potential Azure services/ features that might be potentially relevant:\nVariations\nSee application monitoring.\nWhen to use\nThis solution assumes that your infrastructure monitoring plane is in Azure and that your monitored resources are located in Azure.\nCredits\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:27 UTC\n"},{"dirname":"monitoring_openTelemetry","id":8,"path":"target/generated-docs/solutions/monitoring_openTelemetry/index.html","type":"solution","title":"Monitoring your microservices with openTelemetry","body":"\nMonitoring your microservices with openTelemetry\nWhere is the problem?\nWith the complexity and size of microservice systems, which include hundreds of small services, keeping an overview can be quite challenging. Usually one monolitic application can be scanned for potential errors, as failures in the system directly reference this one application. With microservices the area to search in could be narrowed down, but never to an extend adressing one specific service being the root cause. Therefore, to keep track of runtime problems and the fullfillment of nonfunctional reqirements corresponding to specififc microservices advanced monitoring needs emerge.\nThere are different proposals for collecting this telemetry data consisting of logs, traces and metrics with each proposal having their own benefits and disadvantages leading to a heterogenous landscape of these solutions special for each microservice. As these are configured on their own and little to no replacability options without nudging big change efforts are given, flexibility is limited blocking possibly better solutions. Concluding, overall and in the microservice teams there is not a central repository for looking up your microservices telemetry data but again a numerous amount of different hardly replacable solutions for different microservices making it difficult to gain a central overview e.g. via Grafana.\nThe value for the customer\nMonitoring in microservice settings generally benefits nonfunctional requirements such as performance, availability or security through transparent insights, while enabling proactive but also reactive handling of issues. By having all the telemetry data centrally stored and providing exchangeable solutions for the different types of data, no restrictions to design decisions is made and the customer can gain a fast overview over a great amount of microservices.\nIntroducing OpenTelemetry\nOpenTelemetry forms the combination of OpenTracing and OpenCensus collecting different telemetry data (metrics, logs, traces) for understanding the applications behavior and performance. Because of the standardization for processing telemetry data, OpenTelemetry acts as a central collector for whole application landscapes monitoring data, which is able to communicate with replacable logging-, tracing- or metrics-backends without changing configurations in the applications code.\nAddressing your business\nOpenTelemetry can be used in various use cases, but is best suited in the microservice context. Therefore, any business apllication relying on microservices would be a suited domain for applying an OpenTelemetry solution.\nOpenTelemetry architecture\nThe above example shows a reference architecture of multiple hosts (environments/applications).\nTraces and logs are automatically collected at each JAX-RS service. Other additionally defined metrics, traces or logs are also collected.\nThese applications send data directly to a otel-agent configured to use fewer resources. An otel-agent is a collector instance running with the application or on the same host as the application.\nThe agent then forwards the data to a collector that receives data from multiple agents. Collectors on this layer typically are allowed to use more resources and queue more data.\nThe collector then sends the data to the appropriate backend, in the solution provided by Jaeger, Zipkin or VictoriaMetrics. The VictoriaMetrics backend serves metrics while Jaeger and Zipkin are two alternatives for tracing.\nAdditionally all the telemetry data can be visualized in a tool like Grafana.\nThe OpenTelemetry collector is an instance that makes it possible to receive telemetry data, optionally transform it and send the data on. Receiving, transforming and sending data is done via pipelines. A pipeline consists of a set of receivers, a set of optional processors and a set of exporters.\nA reveiver transfers the telemetry data into the collector, which then can be processed on a processor. Finally, an exporter can send the data to a corresponding backend/destination. Further information can be found here\nList of available receivers, processors and exporters:\nreveivers\nprocessors\nexporters\nIn addition, extensions (Health Check, Performance Profiler, zPages) are provided that can be added to the collector to extend the primary functionality of the collector. These do not require direct access to telemetry data and enable additional functionality outside the usual pipeline.\nDeployment\nYou can find a template for an OpenTelemetry Kubernetes deployment in the GitHub repository of this article with deployment files for an OpenTelemetry agent and collector as well as Jaeger, VictoriaMetrics, and Grafana.\nFor an example of how to use these files, see the devon4j Quarkus reference application.\nRelated documentations\nOpenTelemetry Collector\nOpenTelemetry Java documentation\nJaeger\nZipkin\nVictoriaMetrics\nGrafana\nRelated Architectures and Alternatives\nopenTracing (-OpenTracing has been Archived-)\nopenCensus\n"},{"dirname":"provisioning_azure_azuredevops","id":9,"path":"target/generated-docs/solutions/provisioning_azure_azuredevops/index.html","type":"solution","title":"Provisioning Solutions - Azure DevOps","body":"\nTable of Contents\nProvisioning Solutions - Azure DevOps\nProvisioning Introduction\nContext &amp; Problem\nStandard Problems\nProvisioning Platforms\nAzure\nProvisioning Solutions - Azure DevOps\nOverview\nPattern Details\nVariations\nWhen to use\nCredits\nProvisioning Solutions - Azure DevOps\nProvisioning Introduction\nContext &amp; Problem\nThis document describes patterns to automate the deployment of an application to a target environment. The major players in this documentation are (1) the process as a whole (aka provisioning), (2) automation code and (3) application/ infra code to be deployed. Additionally a third aspect is cross functionalities (3) that affect the automation and the application/infra code such as compliance.\nAutomation code:\nThe automation code is about automating parts of deployment cycle. In an ideal world this covers the entire code development cycle from opening the first branch to the final deployment. Possible activities in this cycle are:\nquality gates\nbuild\ndeployment\nQuality gates refer to testing or approval. Approval might include a manual approval to deploy to sensitive environments like production or enforcing a review before the commit through a pull request. Failing a quality gate should stop the workflow to proceed. Ideally they are maximizing application code coverage and kick in as early as possible.\nFrom a timeline perspective major events that are kicking off actions from the automation code are:\nPull request/ commit (Only triggers quality gates that don’t take too long)\nBuild\nThey can be manually, scheduled (e.g. as part of a nightly build) or automatically started. Additional quality gates ensure code quality.\nDeplyoment to target environment\nA typical quality gate for sensible environments such as production are manual approvals.\nThe major construct is a pipeline that implements a certain activity or a combination. The trigger defines the condition that kick offs a pipeline. Kicking off a pipeline usually includes parameters such as the name of the target environment. The support of different triggers is essental to cover the entire lifecycle. Pipelines can be implemented using a UI driven or programatic approach.\nPipelines are built in a modular way which also adds intermediate steps such as placing the built output in a build artefact repository for later deployment. It also introduces the need to orchestrate them into larger workflows such as creating an entire environment from scratch.\nPipelines must ensure traceability of the performed actions across the entire chain including source code repo, the targeted environment or intermediate stores. This includes a versioning schema for built artefacts. Branches in source repos must be tagged to be able to reconstruct the code behind a versioned artefact.\nApplication/ infra code:\nThe base for automation code activities is the code creating or updating the infrastructure and the application on top. Focus in this documentation is the interaction between automation code and application/ infra code. This includes:\nProgramming approach (Language or UI) for deploying infrastructure and application code\nInteraction standard problems between automation code and application/ infra code\nProblems also depend on the chosen programing approach.\nNot relevant are:\nApplication code: Structuring in repo, programming language specifics and details of build mechanism\nSpecific deployment options of the platform services forming the infrastructure\nProvisioning\nThe process must work in a compliant way end-to-end. Compliance includes the general concern security and to adhere to constraints from the organization’s perspective that need to be enforced.\nProvisoning should be subject to monitoring and must be able to adapt to the organizational structure.\nConfiguration is also a cross concern that affects both sides. From the automation side this includes properties of the environment and settings of the involved automation code. The application specific settings are out of scope in this pattern description. However, provisioning must enable the application code to access settings.\nAs shown automation does not only include pipelines but also additional configuration settings. Automation code must be able to access the infra/ app code for deployment. This pattern includes guidelines regarding options to store the code and how to structure it in a repository. Application code structure is treated as black box.\nIn most cases enterprises have already existing technologies in place that might not be based on cloud of a certain provider. Integration with other third party offerings to cover functionally also from elsewhere is also important.\nThe picture below summarizes the major aspects:\nStandard Problems\nThe following standard problems will be addressed in subsequent paragraphs:\nAutomation code\nRegarding pipelines the folloqing aspects will be detailed:\nModeling environments\nPipeline Implementation (Programing approaches, Triggers, Paramterization, Store Code, Quality Gates)\nOrchestration\nTraceability (Versioning, Tagging)\nInfra/ Application code (Programming approach,\nInteraction standard problems)\nProvisioning\nOrganizational Mapping\nIntegration\nCode Repository (Automation/ Infra &amp; App Code)\nConfiguration\nCompliance\nFor the following aspectcs check out the other defined patterns:\nMonitoring infrastructure and application code\nGeneral guidelines for structuring repositories (e.g. mono vs. multi-repo)\nGeneral guidelines for defining landing zones\nProvisioning Platforms\nAzure\nOverview\nThis chapter lists major features/ concrete services for provisioning of the Azure platform. This architecture pattern builds on the general problem description for monitoring. The picture below summarizes major services and concepts that are discussed in detail in the next chpater.\nAutomation code\nModelling Environments\nAzure provides the following structural elements to model an environment:\nResource groups: Smallest possible container\nSubscriptions: One subscription can contain many resource groups. With a subscription discounts for dev/ test environments are possible.\nManagement groups: One management group can contain many subscriptions. They can be used enforce policies across subscriptions if environments share common characteristics.\nAn environment can be linked to another environment. Linking an environment to multiple environment is beneficial for addressing cross concerns such as monitoring (Similar to Hub/ Spoke topology in networks).\nPipeline Implementation\nThe programming approach can be either UI driven or programmatic. Pipeline programming languages such as YAML structure the actions to be performed by the pipeline and provide basic mechansims such as downloading code from the repo, parameter handling, stating triggers and triggering other programming languages. These other languages are then used to setup infrastructure such as terraform or deploying application code.\nAzure allows to trigger pipelines upon:\na push to repo\na pull request to repo\na schedule\na pipeline completion\nThe platform allows to pass parameters by various mechanisms to pipelines(Explicit per user input, programmatically). Parameters can be passed by group identifier or explicitly as key value pairs. Complex structured objects as known from object programming languages are not directly possible (Require parsing of files with object structure). Parametrization might be constrained by the used service in certain areas.\nThe platform provides support for quality gates as follows:\nStatic code analysis\nMicrosoft does not provide own tools for static code analysis but allows integration of others.\nAutomated tests (Unit, Integration, End-To-End)\nMicrosoft provides services that include test management e.g. creating test suites with test cases and getting an overview about the results.\nApproval\nAzure services support approval for a certain environments and enforcing pull requests as quality gates.\nThe Azure platform provides the following basic options to store automation code:\nServices that provide repositories\nIntegration of various external code repositories\nOrchestration\nTo orchestrate pipelines the two following basic mechanisms can be used:\nImplicit Chaining\nIn that case the complete workflow is not explicitly coded in a dedicated pipeline. Pipelines are chained implicitly by triggering events. The biggest problem with that approach is the missing single pane of control. The current state in the overall workflow is for instance only implicitly given by the currently running pipeline.\nCreating a dedicated orchestration pipeline\nAn additional pipeline triggers in this scenario other pipelines acting as building blocks. Pipelines can run separately (just run the deployment) or as part of a bigger workflow (e.g. create environment from scratch).\nOrchestration must take dependencies into account. They might result from the deployed code or the scope of the pipeline (scope is e.g. a single microservice and code includes the libraries neede).\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals can cause pipelines to fail since the previously deleted resource still exists in the background.(Even although soft delete is not applicable). Whether Azure really deleted everything depends on the service. For instance Azure API management seemed to be affected by that problem.\nTraceability\nTraceability requires an identifier for referencing artefacts. A standard schema is a semantic version. The platform only supports partial support for number generation such as incrementing numbers. Linking the code in the repo to a certain version depends on used repository.\nInfrastructure/ Application code\nA programming language is either \"declarative\" or \"imperative\". Declarative programming languages state the target state and it is the job of the declarative programming language how to get there. The following rules are applied to achieve that:\ncreate a resource if not there\nupdate an existing resource if different properties\ndelete resource if not there\nImperative programming languages state the how. The internal delta calculation needs to be explicitly programmed here. If possible declarative programming languages are recommended due to automatic delta calculation. Typical case is infrastructure.\nsolution_provisioning_platforms_azure_dec_opt\nProvisioning\nOrganizational Mapping\nThe provisioning must match the organizational requirements of your organization. Azure provides services to model sub units within your organization such as departments, projects and teams.\nIntegration\nPlatform allows a modular approach to outsource certain functionality to third party software such as code repository. Which parts is service specific.\nExternal tools providing pipelines can be integrated in two conceptual ways:\nTrigger automation pipelines from external: This involves the configuration of a CI pipeline in the external tool such as Jenkins and mechanism in the automation service that invokes the CI process when source code is pushed to a repository or a branch.\nRun external pipelines from within the platform: In this approach automation reaches out to an external tool to work with the results.\nConfiguration\nConfiguration for provisioning is required in various areas:\nEnvironment: E.g. name of resource group per potential target environment\nRepository: E.g. relevant repos/ branching\nPipelines: Parameters pipelines run with such as the technical user name or settings required by the built/ deployed code.\nConcrete features used for the above three points depend on the used services. A general storage for sensitive data (keys, secrets, certificates) in Azure is always Azure Key Vault.\nCompliance\nThe standard concept for role-based access controls is called RBAC in Azure. It assigns principals (humans or technical accounts) permissions for a certain resource. Regarding provisioning the following users are relevant:\nTechnical user (service principal) the pipelines are running with\nUsers for administrating the provisioning service\nAzure Active Directory is the central service in Azure that defines and controls all principals (human/ service) per tenant.\nGranularity of roles that can be granted depend on the service. The boundaries in which users exist or permissions can be assigned is also service specific.\nProvisioning Solutions - Azure DevOps\nOverview\nThe cloud native provisioning service in Azure is Azure DevOps formerly known as Team Foundation Server (TFS). Azure DevOps covers the full CI/ CD requiremenzs including project management. Due to its extensive extension features you can also replace features with other alternatives (see also under variations).\nNote: Azure DevOps will be superseeded by GitHub in the long run after Microsoft acquired GitHub. New features will be initially implemented there.\nThe services that (can) complement Azure DevOps:\nAzure Key Vault for storing secrets/ exchange of settings\nAzure App Configuration\nThis service provides settings (key-value pairs) and feature toggles. Native integrations exist for typical application programming languages like .NET/ Java. However native integrations with terraform do not exist and it is also not hardened for sensitive information as key vault. Therefore, it is recommended to use that service as special case for application layer if feature toggles are needed.\nAzure AD\nAzure Active Directory provides the service principal the pipelines run with.\nMonitoring\nAzure DevOps generates metrics to check the health pipelines and displays te state in the Azure DevOps portal. However no built-in forwarding to App Insights independent from the deployed application exists. The following options are available:\nContinous monitoring: Monitoring which assumes Web Applications.\nRest API to read service health information manually as explained here. Check out the pattern monitoring cloudnative how polling and forwarding to azure monitor can be achieved.\naudit streaming is similar to diagnostic settings (In preview as of 21.09.2021). It allows to configure a constant forwarding of telemetry to selected targets as described here.\nStructural elements to model environments\nThe picture illustrates the setup with the major dependencies:\nPattern Details\nGeting Started\nMany aspects influence the setup of the service. Following a top down approach the following decisions have to be made:\nDefine landing zone of the service itself in Azure (out of scope)\nOrganizational mapping\nThis yields the structural components to host provisioning which will be detailed in the next chapter. It introduces the possible components and guidelines for its structuring.\nModelling other outlined aspects across automation, infra/ app code and provisioning\nThe structural components are organizations, teams and projects. A team is a unit that supports many team-configurable tools. These tools help you plan and manage work, and make collaboration easier. Every team owns their own backlog, to create a new backlog you create a new team. By configuring teams and backlogs into a hierarchical structure, program owners can more easily track progress across teams, manage portfolios, and generate rollup data.\nA project in Azure DevOps contains the following set of features:\nBoards and backlogs for agile planning\nPipelines for continuous integration and deployment\nRepos\nThe service comes with hosted git repositories inside that service. You can also use the following external source repositories: Bitbuckt Cloud, GitHub, Any generic git repo, Subversion\nTesting\nAzure DevOps supports the following testing by defining test suites with test cases:\nPlanned manual testing. Manual testing by organizing tests into test plans and test suites by designated testers and test leads.\nUser acceptance testing. Testing carried out by designated user acceptance testers to verify the value delivered meets customer requirements, while reusing the test artifacts created by engineering teams.\nExploratory testing. Testing carried out by development teams, including developers, testers, UX teams, product owners and more, by exploring the software systems without using test plans or test suites.\nStakeholder feedback. Testing carried out by stakeholders outside the development team, such as users from marketing and sales divisions.\nTests can also be integrated in pipelines. Pipelines support a wide range of frameworks/ libraries.\nEach organization contains one or more projects\nYour business structure should act as a guide for the number of organizations, projects, and teams that you create in Azure DevOps. Each organization gets its own free tier of services (up to five users for each service type) as follows. You can use all the services, or choose just what you need to complement your existing workflows.\nAzure Pipelines: One hosted job with 1,800 minutes per month for CI/CD and one self-hosted job\nAzure Boards: Work item tracking and Kanban boards\nAzure Repos: for version control and management of source code and artifacts\nAzure Artifacts: Package management\nTesting: Continuous test integration throughout the project life cycle\nAdding multiple projects makes sense in the following cases (see azure projects):\nTo prohibit or manage access to the information contained within a project to select groups\nTo support custom work tracking processes for specific business units within your organization\nTo support entirely separate business units that have their own administrative policies and administrators\nTo support testing customization activities or adding extensions before rolling out changes to the working project\nTo support an Open Source Software (OSS) project\nAdding teams instead of projects is recommended over projects due to agile culture:\nVisibility: It’s much easier to view progress across all teams\nTracking and auditing: It’s easier to link work items and other objects for tracking and auditing purposes\nMaintainability: You minimize the maintenance of security groups and process updates.\nsolution_provisioning_azure_devops_struct\nRemaining goals (Automation Code)\nThis chapter details how the above conceptual features can be achieved with Azure DevOps pipelines.\nThe pipeline programming approach can be either UI driven or programmatic by using YAML. YAML organizes pipelines into a hierarchy of stages, jobs and tasks. Tasks are the workhorse where activities are implemented. Tasks support scripting languages as stated below. They in turn allow to install additional libraries frameworks from third party providers such as terraform (or you use extensions that give you additional task types). The list below highlights a few YAML points you have to be aware of:\nPassing files/ artefacts between jobs/ pipelines\nPassing between jobs within the same pipeline requires publishing the files as pipeline artefacts and downloading it afterwards. Passing between syntax requires a different syntax and also requires a version.\nVariables\nVariables can have different scopes. A special syntax is required to publish them at runtime and to consume them in a different job (requires declaration). (Link). Various predefined exist.\nObtaining client secret\nScripting languages such as terraform might require the client secret for embedded scripting blocks. However, terraform does not provide a way to get it. The only way was to include an AzureCLI scripting task. Setting the argument \"addSpnToEnvironment\" to true makes the value for scripting languages as environment variable. A script can then publish the variable so that the value is available in the YAML pipeline.\nPipelines that shall be triggered by pushing to the repo state in the trigger element the details like branch when they shall run.\nThe example below shows a scheduled trigger:\n# Disable all other triggers\npr: none\ntrigger: none\n# Define schedule\nschedules:\n# Note: Azure DevOps only understands the limited part of the cron\n# expression below. See this link for further details:\n# https://docs.microsoft.com/en-us/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&amp;tabs=yaml\n# Note: With DevOps organization setting of UTC+1 Berlin,...\n# for a given hour x you have to specify x-2 e.g. 16:00 will be\n# started 18:00 o'clock\n- cron: \"30 5 * * MON,TUE,WED,THU,FRI\"\ndisplayName: Business daily morning creation\nalways: true # also run if no code changes\nbranches:\ninclude:\n- 'refs/heads/master'\nPull request (PR) triggers cause a pipeline to run whenever a pull request is opened with one of the specified target branches, or when changes are pushed to such a pull request. In Azure Repos Git, this functionality is implemented using branch policies. To enable pull request validation in Azure Git Repos, navigate to the branch policies for the desired branch, and configure the Build validation policy for that branch. For more information, see Configure branch policies. Draft pull requests do not trigger a pipeline even if you configure a branch policy. Building pull requests from Azure Repos forks is no different from building pull requests within the same repository or project. You can create forks only within the same organization that your project is part of (see PR triggers).\nTo trigger a pipeline upon the completion of another pipeline, specify the triggering pipeline as a pipeline resource. The following example has two pipelines - app-ci (the pipeline defined by the YAML snippet), and security-lib-ci (the triggering pipeline referenced by the pipeline resource). We want the app-ci pipeline to run automatically every time a new version of security-lib-ci is built.\n# this is being defined in app-ci pipeline\nresources:\npipelines:\n- pipeline: securitylib # Name of the pipeline resource\nsource: security-lib-ci # Name of the pipeline referenced by the pipeline resource\nproject: FabrikamProject # Required only if the source pipeline is in another project\ntrigger: true # Run app-ci pipeline when any run of security-lib-ci completes\nImplicit Chaining for orchestration is possible by using trigger condition. Calling pipelines explicitly is so far only possible with scripting.\nsolution_provisioning_azure_devops_orch\nAs part of the configuration Azure DevOps provides the possibility to provide various settings that are used for development such as enforcing pull requests instead of direct pushes to the repo.\nThe major configuration mechanisms in YAML are variables, parameters and variable groups. Variable groups bundle multiple settings as key value pairs. Parameters are not possible in a variable section (Dynamic inclusion of variable groups is possible via file switching). If they are declared on top level they have to be passed when the pipeline is called programmatically or manually by the user.\nQuality gates can be enforced as follows:\nStatic code analysis:\nVarious tool support exists depending on the programming language.\nAutomated tests (Unit, Integration, End-To-End)\nTests can be included in pipelines via additional libraries and additional previous installment through scripting. The task below uses an Azure CLI task to run tests for terraform:\n- task: AzureCLI@2\ndisplayName: Run terratest\ninputs:\nazureSubscription: ${{parameters.svcConn}}\nscriptType: bash\nscriptLocation: 'inlineScript'\naddSpnToEnvironment: true\ninlineScript: |\n# Expose required settings as environment variables\n# ARM_XXX initialized by task due to addSpnToEnvironment = true\nsubsid=`az account show --query id -o tsv`\necho \"client_id:\"$servicePrincipalId\necho \"client_secret:\"$servicePrincipalKey\necho \"subscription_id:\"$subsid\necho \"tenant_id:\"$tenantId\nexport ARM_SUBSCRIPTION_ID=$subsid\nexport ARM_CLIENT_ID=$servicePrincipalId\nexport ARM_CLIENT_SECRET=$servicePrincipalKey\nexport ARM_TENANT_ID=$tenantId\n# Backend settings\nexport storage_account_name=${{parameters.bkStname}}\nexport container_name=${{parameters.bkCntName}}\nexport key=${{parameters.bkRmKeyName}}\n# Other settings\nexport resource_group_name=${{parameters.rgName}}\n# Switch to directory with tests\npwd\ncd test\n# Testfile must end with \"&lt;your name&gt;_test.go\"\ngo test -v my_test.go\nManual approval e.g. for production\nYAML allows deployments to named environments. Approvers can then be defined for the named environments in the portal what causes the deployment pipeline to wait. However Approval must be done multiple times if you have multiple deplyoment blocks. The example below shows a deployment to the environment \"env-demo\":\njobs:\n- deployment:\ndisplayName: run deploy template\npool:\nvmImage: 'ubuntu-latest'\nenvironment: env-demo\nstrategy:\nrunOnce:\ndeploy:\nsteps:\n# - 1. Download artefact\n- task: DownloadPipelineArtifact@2\ndisplayName: Get artefact\ninputs:\ndownloadPath: '$(build.artifactstagingdirectory)'\nartifact: ${{parameters.pipelineArtifactName}}\nRemaining goals (Provisioning)\nConfiguration settings can be broken down into key value pairs. As already stated key vault is the recommended place for storage. Azure App Configuration and variable groups can reference values in Key Vault. Key Value pairs must be selected in YAML based on the target environment. Switching based on the parameter value is possible by constructing filenames based on the parameter value. The resolved filenam contains then the variable group or the key value pairs. as shown below:\n(1) Main pipeline that requires switching\n...\n# Switch in the pipeline which is implemented in a shared repository\nvariables:\n- template: ./pipelines/configurations/vars-env-single-template.yaml@repo-shared\nparameters: ${{parameters.envName}}\n...\n(2) Shared: Switch to correct configuration file\n...\nparameters:\n- name: envName\ndisplayName: name of environment\ntype: string\n# Load filename with resolved parameter value\nvariables:\n- template: vars-env-def-${{parameters.envName}}-template.yaml\n(3) Shared: Configuration file vars-env-def-dev1.yaml\nvariables:\nenvNamePPE1MainScriptLocation: app/dev\nenvNamePPE1SvcLevel: Full\nenvNamePPE1BranchName: dev\nenvNamePPE1KvEnvName: $(envNameCRST)1\nAzure DevOps can integrate with various external tools. Pipelines can be called from external and allow calling external tools. Various third party tools can be manually installed or used via extensions.\nFor compliance Azure DevOps provides various settings inside Azure DevOps itself and via Azure Active Directory.\nPortal access to Boards, Repos, Pipelines, Artifacts and Test Plans can be controlled through Azure DevOps project settings (Link).\nAzure DevOps supports the following autthentication mechanisms to connect to services and resources in your organization (Link):\nOAuth to generate tokens for accessing REST APIs for Azure DevOps. The Organizations and Profiles APIs support only OAuth.\nSSH authentication to generate encryption keys for using Linux, macOS, and Windows running Git for Windows, but you can’t use Git credential managers or personal access tokens (PATs) for HTTPS authentication.\nPersonal access token (PAT) to generate tokens for:\nAccessing specific resources or activities, like builds or work items\nClients like Xcode and NuGet that require usernames and passwords as basic credentials and don’t support Microsoft account and Azure Active Directory features like multi-factor authentication\nAccessing REST APIs for Azure DevOps\nUser permissions for team members are split in access levels and project permissions inside Azure DevOps. The Basic and higher access levels support full access to all Azure Boards features. Stakeholder access provides partial support to select features, allowing users to view and modify work items without having access to all other features. Additional restrictions are possible by Azure Active Directory settings using conditional access policies and MFA. Azure DevOps honors all conditional access policies 100% for our Web flows. For third-party client flow, like using a PAT with git.exe, IP fencing policies are supported only (no support for MFA policies).\nPermissions to work with repositories can be set under project’s repositories settings which also allows to disable forks. Many forks makes it hard to keep the overview and forking allows to download code into someones private account. Azure DevOps supports creating branch protection policies, which protect the code committed to the main branches (project settings ⇒ repo ⇒ branch policies).\nCompliance affects dealing with sensitive settings. As already stated key vault is the standard service for storing them at runtime. Exports from key vault can only be decrypted in a key vault instance.\nHence, secrets can be stored in a repository in a safe way without having to store the values in plain. Using them later should be done in a safe way. This includes publishing them in a safe way and passing them from YAML to terraform by avoiding log output in plain text. Avoiding log output passing them as environment variables/ files.\nThe following repository structure shows a conceptual breakdown that covers most aspects:\n1. Infra\n1.1. Infrastructure\n1.1.1. Other landing zones\nRepresents other areas with shared functionality that are required. Examples are environments for monitoring, the environment containing Azure DevOps, Key Vault settings etc.\n1.1.2. App Environments\nRepresents the environments where application is deployed to.\n1.1.2.1. Envs\nThis level contains all infrastructure code for seting up en environment. The split between dev and non-dev leverages cost savings for less performant dev environments e.g. by picking cheaper service configurations or totally different Azure services.\n1.1.2.1.1. Dev\n1.1.2.1.2. Non-Dev\n1.1.2.1.3. Modules\nFactored out modules for shared reuse. One example is a central module to generate the name for a given module.\n1.1.2.2. Envs-Mgmt\nCaptures aspects assumed by the chosen programming language such as terraform for managing an environment. This includes for instance the backend creation code.\n1.2. Pipelines\nPipelines for automating infrastrcuture deployment.\n2. App\n2.1. Application (Black Box)\n2.2. Pipelines\nPipelines for automating app code deployment.\n3. Shared\nCaptures shared aspects between infrastructure and application code such as publishing key vault secrets for a pipeline or triggering another pipeline.\nVariations\nThe following features of Azure DevOps can be replaced with alternatives:\nRepos\nBoards for project management\nAzure Artefacts\nThe following alternatives on service level exist:\nAzure Lab Services for Dev/ Test scenarios\nKubernetes\nAzure DevSpaces (Deprecated) in favor of “Bridge-to-kubernetes” for Dev/ Test scenarios\nBridge-to-Kubernetes\nWhen to use\nAzure DevOps makes sense if you want to provision to Azure due to its tight integration into the Azure platform. The following circumstances also speak for Azure DevOps:\nYou need a mature service on enterprise level\nYou don’t need cloud agnostic pipelines e.g. due to a multi-cloud scenario\nYour code repository is not GitHub and Azure DevOps provides integrations for it (GitHub Actions are a better alternative if your code repos are already on GitHub.)\nYou need a full blown provisioning service and plan to use the integrated repos that come with Azure DevOps\nCredits\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:33 UTC\n"},{"dirname":"provisioning_azure_github","id":10,"path":"target/generated-docs/solutions/provisioning_azure_github/index.html","type":"solution","title":"Provisioning Solutions - GitHub","body":"\nTable of Contents\nProvisioning Solutions - GitHub\nProvisioning Introduction\nContext &amp; Problem\nStandard Problems\nProvisioning Platforms\nAzure\nProvisioning Solutions - GitHub\nOverview\nPattern Details\nVariations\nWhen to use\nCredits\nProvisioning Solutions - GitHub\nProvisioning Introduction\nContext &amp; Problem\nThis document describes patterns to automate the deployment of an application to a target environment. The major players in this documentation are (1) the process as a whole (aka provisioning), (2) automation code and (3) application/ infra code to be deployed. Additionally a third aspect is cross functionalities (3) that affect the automation and the application/infra code such as compliance.\nAutomation code:\nThe automation code is about automating parts of deployment cycle. In an ideal world this covers the entire code development cycle from opening the first branch to the final deployment. Possible activities in this cycle are:\nquality gates\nbuild\ndeployment\nQuality gates refer to testing or approval. Approval might include a manual approval to deploy to sensitive environments like production or enforcing a review before the commit through a pull request. Failing a quality gate should stop the workflow to proceed. Ideally they are maximizing application code coverage and kick in as early as possible.\nFrom a timeline perspective major events that are kicking off actions from the automation code are:\nPull request/ commit (Only triggers quality gates that don’t take too long)\nBuild\nThey can be manually, scheduled (e.g. as part of a nightly build) or automatically started. Additional quality gates ensure code quality.\nDeplyoment to target environment\nA typical quality gate for sensible environments such as production are manual approvals.\nThe major construct is a pipeline that implements a certain activity or a combination. The trigger defines the condition that kick offs a pipeline. Kicking off a pipeline usually includes parameters such as the name of the target environment. The support of different triggers is essental to cover the entire lifecycle. Pipelines can be implemented using a UI driven or programatic approach.\nPipelines are built in a modular way which also adds intermediate steps such as placing the built output in a build artefact repository for later deployment. It also introduces the need to orchestrate them into larger workflows such as creating an entire environment from scratch.\nPipelines must ensure traceability of the performed actions across the entire chain including source code repo, the targeted environment or intermediate stores. This includes a versioning schema for built artefacts. Branches in source repos must be tagged to be able to reconstruct the code behind a versioned artefact.\nApplication/ infra code:\nThe base for automation code activities is the code creating or updating the infrastructure and the application on top. Focus in this documentation is the interaction between automation code and application/ infra code. This includes:\nProgramming approach (Language or UI) for deploying infrastructure and application code\nInteraction standard problems between automation code and application/ infra code\nProblems also depend on the chosen programing approach.\nNot relevant are:\nApplication code: Structuring in repo, programming language specifics and details of build mechanism\nSpecific deployment options of the platform services forming the infrastructure\nProvisioning\nThe process must work in a compliant way end-to-end. Compliance includes the general concern security and to adhere to constraints from the organization’s perspective that need to be enforced.\nProvisoning should be subject to monitoring and must be able to adapt to the organizational structure.\nConfiguration is also a cross concern that affects both sides. From the automation side this includes properties of the environment and settings of the involved automation code. The application specific settings are out of scope in this pattern description. However, provisioning must enable the application code to access settings.\nAs shown automation does not only include pipelines but also additional configuration settings. Automation code must be able to access the infra/ app code for deployment. This pattern includes guidelines regarding options to store the code and how to structure it in a repository. Application code structure is treated as black box.\nIn most cases enterprises have already existing technologies in place that might not be based on cloud of a certain provider. Integration with other third party offerings to cover functionally also from elsewhere is also important.\nThe picture below summarizes the major aspects:\nStandard Problems\nThe following standard problems will be addressed in subsequent paragraphs:\nAutomation code\nRegarding pipelines the folloqing aspects will be detailed:\nModeling environments\nPipeline Implementation (Programing approaches, Triggers, Paramterization, Store Code, Quality Gates)\nOrchestration\nTraceability (Versioning, Tagging)\nInfra/ Application code (Programming approach,\nInteraction standard problems)\nProvisioning\nOrganizational Mapping\nIntegration\nCode Repository (Automation/ Infra &amp; App Code)\nConfiguration\nCompliance\nFor the following aspectcs check out the other defined patterns:\nMonitoring infrastructure and application code\nGeneral guidelines for structuring repositories (e.g. mono vs. multi-repo)\nGeneral guidelines for defining landing zones\nProvisioning Platforms\nAzure\nOverview\nThis chapter lists major features/ concrete services for provisioning of the Azure platform. This architecture pattern builds on the general problem description for monitoring. The picture below summarizes major services and concepts that are discussed in detail in the next chpater.\nAutomation code\nModelling Environments\nAzure provides the following structural elements to model an environment:\nResource groups: Smallest possible container\nSubscriptions: One subscription can contain many resource groups. With a subscription discounts for dev/ test environments are possible.\nManagement groups: One management group can contain many subscriptions. They can be used enforce policies across subscriptions if environments share common characteristics.\nAn environment can be linked to another environment. Linking an environment to multiple environment is beneficial for addressing cross concerns such as monitoring (Similar to Hub/ Spoke topology in networks).\nPipeline Implementation\nThe programming approach can be either UI driven or programmatic. Pipeline programming languages such as YAML structure the actions to be performed by the pipeline and provide basic mechansims such as downloading code from the repo, parameter handling, stating triggers and triggering other programming languages. These other languages are then used to setup infrastructure such as terraform or deploying application code.\nAzure allows to trigger pipelines upon:\na push to repo\na pull request to repo\na schedule\na pipeline completion\nThe platform allows to pass parameters by various mechanisms to pipelines(Explicit per user input, programmatically). Parameters can be passed by group identifier or explicitly as key value pairs. Complex structured objects as known from object programming languages are not directly possible (Require parsing of files with object structure). Parametrization might be constrained by the used service in certain areas.\nThe platform provides support for quality gates as follows:\nStatic code analysis\nMicrosoft does not provide own tools for static code analysis but allows integration of others.\nAutomated tests (Unit, Integration, End-To-End)\nMicrosoft provides services that include test management e.g. creating test suites with test cases and getting an overview about the results.\nApproval\nAzure services support approval for a certain environments and enforcing pull requests as quality gates.\nThe Azure platform provides the following basic options to store automation code:\nServices that provide repositories\nIntegration of various external code repositories\nOrchestration\nTo orchestrate pipelines the two following basic mechanisms can be used:\nImplicit Chaining\nIn that case the complete workflow is not explicitly coded in a dedicated pipeline. Pipelines are chained implicitly by triggering events. The biggest problem with that approach is the missing single pane of control. The current state in the overall workflow is for instance only implicitly given by the currently running pipeline.\nCreating a dedicated orchestration pipeline\nAn additional pipeline triggers in this scenario other pipelines acting as building blocks. Pipelines can run separately (just run the deployment) or as part of a bigger workflow (e.g. create environment from scratch).\nOrchestration must take dependencies into account. They might result from the deployed code or the scope of the pipeline (scope is e.g. a single microservice and code includes the libraries neede).\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals can cause pipelines to fail since the previously deleted resource still exists in the background.(Even although soft delete is not applicable). Whether Azure really deleted everything depends on the service. For instance Azure API management seemed to be affected by that problem.\nTraceability\nTraceability requires an identifier for referencing artefacts. A standard schema is a semantic version. The platform only supports partial support for number generation such as incrementing numbers. Linking the code in the repo to a certain version depends on used repository.\nInfrastructure/ Application code\nA programming language is either \"declarative\" or \"imperative\". Declarative programming languages state the target state and it is the job of the declarative programming language how to get there. The following rules are applied to achieve that:\ncreate a resource if not there\nupdate an existing resource if different properties\ndelete resource if not there\nImperative programming languages state the how. The internal delta calculation needs to be explicitly programmed here. If possible declarative programming languages are recommended due to automatic delta calculation. Typical case is infrastructure.\nsolution_provisioning_platforms_azure_dec_opt\nProvisioning\nOrganizational Mapping\nThe provisioning must match the organizational requirements of your organization. Azure provides services to model sub units within your organization such as departments, projects and teams.\nIntegration\nPlatform allows a modular approach to outsource certain functionality to third party software such as code repository. Which parts is service specific.\nExternal tools providing pipelines can be integrated in two conceptual ways:\nTrigger automation pipelines from external: This involves the configuration of a CI pipeline in the external tool such as Jenkins and mechanism in the automation service that invokes the CI process when source code is pushed to a repository or a branch.\nRun external pipelines from within the platform: In this approach automation reaches out to an external tool to work with the results.\nConfiguration\nConfiguration for provisioning is required in various areas:\nEnvironment: E.g. name of resource group per potential target environment\nRepository: E.g. relevant repos/ branching\nPipelines: Parameters pipelines run with such as the technical user name or settings required by the built/ deployed code.\nConcrete features used for the above three points depend on the used services. A general storage for sensitive data (keys, secrets, certificates) in Azure is always Azure Key Vault.\nCompliance\nThe standard concept for role-based access controls is called RBAC in Azure. It assigns principals (humans or technical accounts) permissions for a certain resource. Regarding provisioning the following users are relevant:\nTechnical user (service principal) the pipelines are running with\nUsers for administrating the provisioning service\nAzure Active Directory is the central service in Azure that defines and controls all principals (human/ service) per tenant.\nGranularity of roles that can be granted depend on the service. The boundaries in which users exist or permissions can be assigned is also service specific.\nProvisioning Solutions - GitHub\nOverview\nGitHub was recently acquired by Microsoft. Microsoft announced that new features will be first incorporated into GitHub instead of Azure DevOps. This announcements shows the future strategic shift from Microsoft away from Azure DevOps to GitHub. The subset of features for implementing pipelines is called GitHub Actions. A pipeline is also called a workflow. So far GitHub Actions are limited to repositories within GitHub ((Running GitHub Actions in Azure requires manual installation of the required software on an Azure VM).\nAzure Key Vault for storing secrets/ exchange of settings\nKey vault secrets can be included by importing them as GitHub secrets as described here.\nAzure App Configuration\nThis service provides settings (key-value pairs) and feature toggles. Native integrations exist for typical application programming languages like .NET/ Java. However native integrations with terraform do not exist and it is also not hardened for sensitive information as key vault. Therefore, it is recommended to use that service as special case for application layer if feature toggles are needed.\nChanges at in the repo can be automatically pushed to an App Configuration instance as described here.\nAzure AD\nAzure Active Directory provides the service principal the pipelines run with. The essential values are stored as GitHub secrets. From there you can use them as in your workflows as described here.\nMonitoring\nGitHub Actions generates metrics to check the health pipelines and displays te state in the Azure DevOps portal. GitHub Actions provides the Checks API to output statuses, results, and logs for a workflow which you can use for download as described here. An azure service without much overhead would be Azure Automation which allows to store and run scripts for polling.\nStructural elements to model environments\nThe picture illustrates the setup with the major dependencies:\nPattern Details\nGeting Started\nMany aspects influence the setup of the service. Following a top down approach the following decisions have to be made:\nOrganizational mapping\nThis yields the structural components to host provisioning which will be detailed in the next chapter. It introduces the possible components and guidelines for its structuring.\nModelling other outlined aspects across automation, infra/ app code and provisioning\nThe structural components are organizations, teams and projects (in public beta as of 19.09.2021).\nOrganizations are a group of two or more users that typically mirror realworld organizations. They are administered by Organization members and can contain both repositories and teams.\nA project is a customizable spreadsheet that integrates with your issues and pull requests on GitHub. You can customize the layout by filtering, sorting, and grouping your issues and PRs. You can also add custom fields to track metadata. Projects are flexible so that your team can work in the way that is best for them.\nTeams give you the ability to create groups of organization members with read, write, or admin permissions to repositories that belong to the organization. Teams are also central to many of GitHub’s collaborative features, such as team @mentions, which notify appropriate groups of people that you’d like their input or attention. Teams can be both project or subject-matter focused, and can relate to job titles or interest groups within your company as well.\nWhen setting up your GitHub Enterprise instance, the immediate instinct may be to create an Organization for every project or department at your company, leading to many divided groups that function in GitHub as siloes. This may seem like a good way to manage permissions and reduce noise, but it’s not always the ideal strategy. In fact, it is detrimental to cross-team collaboration and can result in administrative headaches down the line. Common setups are shown below:\nSetup Type\nCompany size\nBenefits\nSingle Org and team\nsmall, potentially medium\nideal for highly collaborative start up\nSinge Org, multiple teams\nmedium/ small companies with strict security needs\nmore granular repo access\nMultiple Orgs and multiple teams\nLarge companys with strict repo access\nhigher level of separation, best for companys with &gt;500 users\nRemaining goals (Automation Code)\nThis chapter details how the above conceptual features can be achieved with Azure DevOps pipelines.\nGithub only allows a programming approach for pipelines. YAML organizes pipelines into a hierarchy jobs and steps (a step is also refered to as action). Steps are the workhorse where activities are implemented. Steps support scripting languages as stated below. They in turn allow to install additional libraries frameworks from third party providers such as terraform (or you use extensions that give you additional task types). The list below highlights a few YAML points you have to be aware of:\nPassing files/ artefacts between jobs/ pipelines\nArtefacts can be passed between pipelines by uploading and downloading them from GitHub as shown here.\nVariables\nVariables can have different scopes. Two ways are possible to pass variables between steps (set-env, set as output). Direct support for enum like variables do not yet exist or workarounds are needed as described here.\nVariables can be simple key value pairs only. Values can be string and other basic types such as bool. Strings can contain structured json to accommodate more complex structures as shown below:\non:\npush:\njobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- name: set output\nid: set\nrun: |\necho ::set-output name=json_var::'[{ \"name\": \"test\", \"client_payload\": \"111\" }, { \"name\": \"test2\", \"client_payload\": \"222\" }] '\n- name: use output\nrun: |\necho $json_var | jq '.[].name'\nenv:\njson_var: ${{ steps.set.outputs.json_var}}\nObtaining client secret\nScripting languages such as terraform might require the client secret for embedded scripting blocks. Due to the direct encoding as GitHub secrets this is not a problem.\nTriggers\nYou can configure your workflows to run when specific activity on GitHub happens, at a scheduled time, or when an event outside of GitHub occurs as described\nhere.\nImplicit Chaining for orchestration is possible by using trigger condition. Calling other workflows explicitly is so far only possible with scripting as shown here here.\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals might cause pipelines to fail. Even if resources are deleted they might still exist in the background (even although soft delete is not applicable). Programming languages can therefore get confused if pipelines recreate things in short intervals. Creating a new resource group can solve the problem since they are part of the tecnical resource id.\nAs part of the configuration GitHub Actions provide the following configuration mechanisms:\nWorkflow input parameters\nWhen using the workflow_call keyword, you can optionally specify inputs that are passed to the called workflow from the caller workflow. Inputs for reusable workflows are specified with the same format as action inputs.\non:\nworkflow_call:\ninputs:\nusername:\ndescription: 'A username passed from the caller workflow'\ndefault: 'john-doe'\nrequired: false\ntype: string\njobs:\nprint-username:\nruns-on: ubuntu-latest\nsteps:\n- name: Print the input name to STDOUT\nrun: echo The username is ${{ inputs.username }}\nAction can use variables as input. Outputs (=string) of a step/ job can be used in subsequent steps/ jobs.\nEnvironments\nEnvironments can hold with protection rules such as manual approval and secrets. A workflow job can reference an environment to use the environment’s protection rules and secrets. The environment name can be set dynamically in scripts as shwon here.\nGitHub Actions includes a collection of variables called contexts and a similar collection of variables called default environment variables.\nDefault environment variables exist only on the runner that is executing your job.\nMost contexts you can use at any point in your workflow, including when default environment variables would be unavailable. For example, you can use contexts with expressions to perform initial processing before the job is routed to a runner for execution; this allows you to use a context with the conditional if keyword to determine whether a step should run.\nSecrets are encrypted environment variables that you create in an organization, repository, or repository environment. The secrets that you create are available to use in GitHub Actions workflows.\nQuality gates can be enforced as follows:\nStatic code analysis:\nVarious tool support exists depending on the programming language such as SonarCube.\nAutomated tests (Unit, Integration, End-To-End)\nTests can be included in pipelines via additional libraries and additional previous installment through scripting. The workflow below runs npm tests:\njobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- name: Check out code\n- uses: actions/checkout@v2\n- name: Set up node\nuses: actions/setup-node@v1\n- name: Install dependencies\nrun: npm install\n- name: Run tests\nrun: npm test\nManual approval e.g. for production\nGitHub actions allows deployments to named environments. Approvers can then be added as environments protection rules. The terraform apply command below is bound to the environment production:\nterraformapply:\nname: 'Terraform Apply'\nneeds: [terraform]\nruns-on: ubuntu-latest\nenvironment: production\nRemaining goals (Provisioning)\nGitHUb Actions can integrate with various external tools. Pipelines can be called from external (see here) and allow calling external tools. Various third party tools can be manually installed or used via extensions.\nFor compliance GitHub provides various settings as described here.\nSecrets can be configured at the organization, repository, or environment level, and allow you to store sensitive information in GitHub. They should not contain structured content like JSON since they are reacted to avoid display in logs.\nYou can use the CODEOWNERS feature to control how changes are made to your workflow files. For example, if all your workflow files are stored in .github/workflows, you can add this directory to the code owners list, so that any proposed changes to these files will first require approval from a designated reviewer.\nYou should ensure that untrusted input does not flow directly into workflows, actions, API calls, or anywhere else where they could be interpreted as executable code. In addition, there are other less obvious sources of potentially untrusted input, such as branch names and email addresses, which can be quite flexible in terms of their permitted content. For example, zzz\";echo${IFS}\"hello\";# would be a valid branch name. A pull request with title of a\"; ls $GITHUB_WORKSPACE\" would for instance list the directory if the workflow would be as follows:\n- name: Check PR title\nrun: |\ntitle=\"${{ github.event.pull_request.title }}\"\nif [[index.asciidoc_ $title =~ ^octocat ]]; then\necho \"PR title starts with 'octocat'\"\nexit 0\nelse\necho \"PR title did not start with 'octocat'\"\nexit 1\nfi\nTo help you manage the risk of dangerous patterns as early as possible in the development lifecycle, the GitHub Security Lab has developed CodeQL queries that repository owners can integrate into their CI/CD pipelines. This action runs GitHub’s industry-leading semantic code analysis engine, CodeQL, against a repository’s source code to find security vulnerabilities. It then automatically uploads the results to GitHub so they can be displayed in the repository’s security tab.\nActions can use the GITHUB_TOKEN by accessing it from the github.token context. It’s good security practice to set the default permission for the GITHUB_TOKEN to read access only for repository contents.\nThe following repository structure shows a conceptual breakdown that covers most aspects:\n1. Infra\n1.1. Infrastructure\n1.1.1. Other landing zones\nRepresents other areas with shared functionality that are required. Examples are environments for monitoring, the environment containing Azure DevOps, Key Vault settings etc.\n1.1.2. App Environments\nRepresents the environments where application is deployed to.\n1.1.2.1. Envs\nThis level contains all infrastructure code for seting up en environment. The split between dev and non-dev leverages cost savings for less performant dev environments e.g. by picking cheaper service configurations or totally different Azure services.\n1.1.2.1.1. Dev\n1.1.2.1.2. Non-Dev\n1.1.2.1.3. Modules\nFactored out modules for shared reuse. One example is a central module to generate the name for a given module.\n1.1.2.2. Envs-Mgmt\nCaptures aspects assumed by the chosen programming language such as terraform for managing an environment. This includes for instance the backend creation code.\n1.2. Pipelines\nPipelines for automating infrastrcuture deployment.\n2. App\n2.1. Application (Black Box)\n2.2. Pipelines\nPipelines for automating app code deployment.\n3. Shared\nCaptures shared aspects between infrastructure and application code such as publishing key vault secrets for a pipeline or triggering another pipeline.\nVariations\nPossible Other Third Party\nFor the following features other tools can be used:\nProject management support can be added by using other tools such as Azure DevOps.\nArtefacts can be stored also in other systems\nMigrate from Azure DevOps\nWorkflows/ Repos can be created by porting from Azure DevOps. Besides Azure DevOps pipelines and Azure DevOps Repos additional features/ settings exist, which need to be considered such as Azure DevOps artefacts. The picture below summarizes the starting point.\nThe migration approach detailed below tries to fulfill the following constraints:\nNo big bang introduction of GitHub Actions but incremental approach\nMinimizing parallel infrastructure in GitHub until migration is fully finished\nTo account for an incremental approach the pipelines are migrated first and the repo including its settings afterwards. The new GitHub workflow is created on the Azure DevOps side and copied over to GitHub Actions for execution. This helps to minimize parallel infrastructure in GitHub until Azure DevOps repos are fully migrated. The picture below illustrate the first stage (Placement of remote executioner is just one possible example):\nIn the second step the Azure DevOps Repo is moved to GitHub. All related settings are migrated. Not required components such as the remote executioner can be dropped. The picture below illustrate the second stage:\nThe subsequent paragraphs detail the introduced two steps:\nMigrate Azure DevOps Pipelines to GitHub workflows\nAzure DevOps Pipeline can call GitHub Workflows. This can be used to recreate GitHub Workflows without adjusting the interface of the Azure DevOps pipelines. Tools exist that allow at least partially to automatically translate the yaml code.\nAzure DevOps Artefacts can be migrated to GitHub artefacts, that can be also uploaded and downloaded for reuse.\nMigrate Azure DevOps Repos to GitHub\nAn Azure DevOps repo can be imported into GitHub as described here.\nWhen to use\nUsing GitHub makes sense in the following scenarios:\nYou need cloud agnostic pipelines e.g. due to a multi-cloud scenario\nYour code repository is GitHub and absence of projects for project management is not a problem or can be replaced with something else such as Azure DevOps\nCredits\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:37 UTC\n"},{"dirname":"security_authentication","id":11,"path":"target/generated-docs/solutions/security_authentication/index.html","type":"solution","title":"Authentication","body":"\nAuthentication\nAccess control is an important aspect for the security in IT application landscapes. There are two different aspects to distinguish:\nAuthentication (Who tries to access?)\nAuthorization (Is the one accessing allowed to do what he wants to do?)\nThis part deals with the recommendations on authentication.\nYou have the following problem to be solved\nIn large IT landscapes it is a highly recommended best-practice to centralize your authentication. Implemeting the actual authentication into every application or service is therefore considered as an anti-pattern. Instead we suggest to use a central identity and access management (IAM) solution based on established products (e.g. Keycloak).\nUsing a central IAM\nWhen using a central IAM, the user is redirected to the identity provider (IdP) when trying to access the application. The IdP returns a login page where the user can log in. After confirming that the user can access the application, the IdP returns an access token that the user can use to access the application. We recommend the use JSON Web Tokens (JWT) within the authentication flow, as this is a widely used method in modern web applications and RESTful services.\nParticipants integrate with the identity provider using protocols such as OpenId Connect, SAMLv2 or WebAuthn. The original incoming request is forwarded to the actual service and the token is added as a bearer token via HTTP header according to OAuth standard. In the application, the token can be validated according to the user’s roles and groups.\nTypically, a gateway is placed in front of the IdP and applications to act as a reverse proxy for the actual service. Incoming traffic goes through the gateway and the gateway is then responsible for authentication through integration with the identity provider. In this way, multiple applications and services can be deployed without implementing integration between the service itself and the IdP. The access token can be validated only on the gateway side or additionally passed to the application for further validation.\nServices are implemented stateless and only accept requests with valid JWT from gateway. When one of your services invokes another service it simply passes on the JWT via HTTP header. This way all sub-sequent invocations happen within the context and with the permissions of the initial user.\nThe gateway should also act as a portal that integrates the UIs of your microservices so that end users do not notice which UI comes from which service, but have the user experience (UX) of a single monolithic UI.\nWhich protocol to use\nWe suggest using OIDC as the protocol for integration with the identity provider. It is easy to integrate and works well with mobile and web-based applications. OIDC uses JSON tokens and RESTful APIs to provide the authentication information. Therefore, it is a much more lightweight solution than SAML, which uses an XML and SOAP-based approach.\nValues for the customer\nall services are independent and decoupled from the actual authentication and IAM\nauthentication can be changed without touching any of your services, only changes need to be made to your gateway(s)\nin large and complex IT landscapes, there may be different requirements for authentication via different channels (e.g. to authenticate internal users via SPNEGO and external users via WebAuthn). In such a case, you can simply set up several variants of your gateway for each channel with different endpoint URLs.\nConventions\nWe recommend the following conventions:\ndefine a short but meaningful unique alphanumeric identifier for each of your services (app-id)\nestablish a clear URL scheme for accessing your apps, e.g. https://gateway.company.com/«app-id»/\nuse a cloud infrastructure platform that allows to manage an overlay network so you can configure loadbalancers or even a service-mesh mapping your service entry points to a consistent URL schema such as https://«app-id»:8443/«app-id»/\nthis way you do not need any configuration or business knowledge inside your gateway as the routing can be implemented fully generic\nuse app-id. as a prefix to all permission groups/roles specific to your service to avoid name clashing in your central IAM\nImplementation hints\nA Helm chart for the deployment of Keycloak in your Kubernetes environment can be found in the GitHub repository of the devonfw solutions browser\nThe devon4quarkus-product reference application provides a documentation on how to integrate Keycloak into your Quarkus application to implement security mechanisms\nRelated documentations\ndevon4j authentication guide\ndevon4j JWT guide\nOAuth 2.0\nOpenID Connect\nKeycloak’s securing apps guide\nIAM solutions\nKeycloak\nWSO2\nGluu Server\nForgeRock\n…​\n"},{"dirname":"security_authorization","id":12,"path":"target/generated-docs/solutions/security_authorization/index.html","type":"solution","title":"Authorization","body":"\nAccess control is an important aspect for the security in IT application landscapes. There are two different aspects to distinguish: authentication and authorization.\nThis part deals with the recommendations on authorization, concretely with the general concept and convention. Details how to implement this with specific libraries, or programming-languages are described in the individual stacks of devonfw.\nAuthorization\nDefinition:\nAuthorization is the verification that an authenticated user is allowed to perform the operation he intends to invoke.\nClarification of terms:\nFor clarification we also want to give a common understanding of related terms that have no unique definition and consistent usage in the wild.\nTable 1. Security terms related to authorization\nTerm\nMeaning and comment\nPrincipal\nAn entity that can be authenticated e.g. a user, an application\nPermission\nA permission is an object that allows a principal to perform an operation in the system. This permission can be granted (give) or revoked (taken away). Sometimes people also use the term right what is actually wrong as a right (such as the right to be free) can not be revoked.\nGroup\nWe use the term group in this context for an object that contains permissions. A group may also contain other groups. Then the group represents the set of all recursively contained permissions.\nRole\nWe consider a role as a specific form of group that also contains permissions. A role identifies a specific function of a principal. A user can act in a role.For simple scenarios a principal has a single role associated. In more complex situations a principal can have multiple roles but has only one active role at a time that he can choose out of his assigned roles. For KISS it is sometimes sufficient to avoid this by creating multiple accounts for the few users with multiple roles. Otherwise at least avoid switching roles at run-time in clients as this may cause problems with related states. Simply restart the client with the new role as parameter in case the user wants to switch his role.\nAccess Control\nAny permission, group, role, etc., which declares a control for access management.\nSuggestions on the access model\nFor the access model we give the following suggestions:\nEach Access Control (permission, group, role, …​) is uniquely identified by a human readable string.\nWe create a unique permission for each use-case.\nWe define groups that combine permissions to typical and useful sets for the users.\nWe define roles as specific groups as required by our business demands.\nWe allow to associate users with a list of Access Controls.\nFor authorization of an implemented use case we determine the required permission. Furthermore, we determine the current user and verify that the required permission is contained in the tree spanned by all his associated Access Controls. If the user does not have the permission we throw a security exception and thus abort the operation and transaction.\nWe avoid negative permissions, that is a user has no permission by default and only those granted to him explicitly give him additional permission for specific things. Permissions granted can not be reduced by other permissions.\nTechnically we consider permissions as a secret of the application. Administrators shall not fiddle with individual permissions but grant them via groups. So the access management provides a list of strings identifying the Access Controls of a user. The individual application itself contains these Access Controls in a structured way, whereas each group forms a permission tree.\nDo not use the pattern that defines non-configured permission as no limitation or in other word all permissions.\n[DB1,DB2] → allow to access DB1 and DB2\n[] → have no permission at all → good\n[] → have all permissions → bad\nNaming conventions\nAs stated above each Access Control is uniquely identified by a human readable string. This string should follow the naming convention:\n«app-id».«local-name»\nFor Access Control Permissions the «local-name» again follows the convention:\n«verb»«object»\nThe segments are defined by the following table:\nTable 2. Segments of Access Control Permission ID\nSegment\nDescription\nExample\n«app-id»\nIs a unique technical but human readable string of the application (or microservice). It shall not contain special characters and especially no dot or whitespace. We recommend to use lower-train-case-ascii-syntax. The identity and access management should be organized on enterprise level rather than application level. Therefore permissions of different apps might easily clash (e.g. two apps might both define a group ReadMasterData but some user shall get this group for only one of these two apps). Using the «app-id». prefix is a simple but powerful namespacing concept that allows you to scale and grow. You may also reserve specific «app-id»s for cross-cutting concerns that do not actually reflect a single app e.g to grant access to a geographic region.\nshop\n«verb»\nThe action that is to be performed on «object». We use Find for searching and reading data. Save shall be used both for create and update. Only if you really have demands to separate these two you may use Create in addition to Save. Finally, Delete is used for deletions. For non CRUD actions you are free to use additional verbs such as Approve or Reject.\nFind\n«object»\nThe affected object or entity. Shall be named according to your data-model\nProduct\nSo as an example shop.FindProduct will reflect the permission to search and retrieve a Product in the shop application. The group shop.ReadMasterData may combine all permissions to read master-data from the shop. However, also a group shop.Admin may exist for the Admin role of the shop application. Here the «local-name» is Admin that does not follow the «verb»«object» schema.\nData permissions\nIn some projects there are demands for permissions and authorization that is dependent on the processed data. E.g. a user may only be allowed to read or write data for a specific region. This is adding some additional complexity to your authorization. If you can avoid this it is always best to keep things simple. However, in various cases this is a requirement. Please clarify the following questions before you make your decisions how to design your access controls:\nDo you need to separate data-permissions independent of the functional permissions? E.g. may it be required to express that a user can read data from the countries ES and PL but is only permitted to modify data from PL? In such case a single assignment of \"country-permissions\" to users is insufficient.\nDo you want to grant data-permissions individually for each application (higher flexibility and complexity) or for the entire application landscape (simplicity, better maintenance for administrators)? In case of the first approach you would rather have access controls like app1.country.GB and app2.country.GB.\nDo your data-permissions depend on objects that can be created dynamically inside your application?\nIf you want to grant data-permissions on other business objects (entities), how do you want to reference them (primary keys, business keys, etc.)? What reference is most stable? Which is most readable?\nIf data-permission is the way to go, please checkout our guidance and patterns how to solve this properly.\nImplementation hints\nAccess control\nData permission\n"},{"dirname":"streamproc_azure_kafka","id":13,"path":"target/generated-docs/solutions/streamproc_azure_kafka/index.html","type":"solution","title":"Stream Processing Solutions - Apache Kafka on Microsoft Azure","body":"\nTable of Contents\nStream Processing Solutions - Apache Kafka on Microsoft Azure\nStream Processing Introduction\nContext &amp; Problem\nStandard Problems\nStream Processing Platforms\nAzure\nProvisioning Solutions - Apache Kafka on Microsoft Azure\nEvent Hub\nAKS\nConfluent Cloud\nHDInsight\nComparison\nCredits\nStream Processing Solutions - Apache Kafka on Microsoft Azure\nTable of Contents\nStream Processing Solutions - Apache Kafka on Microsoft Azure\nStream Processing Introduction\nContext &amp; Problem\nStandard Problems\nStream Processing Platforms\nAzure\nProvisioning Solutions - Apache Kafka on Microsoft Azure\nEvent Hub\nAKS\nConfluent Cloud\nHDInsight\nComparison\nCredits\nStream Processing Introduction\nContext &amp; Problem\nStream processing is a method of tracking and analyzing (processing) streams of information (data) about things that happen (events) and deriving a conclusion from them. Related words also referring to the core of that technology are: Complex Event Processing (CEP) or Command and Query Responsibility Segregation (CQRS). Due to its benefits (see below) its also a vital technology in conjunction with real-time processing.\nThe foundation of modern stream processing software is a data structure called an append-only log. A log is just a sequence of binary records. An append-only log only allows for the addition of new elements to the end of the log. We can’t insert new elements at arbitrary positions of the log, and we can’t remove elements at will. Every element in a log has a sequence number, and newer elements have a higher sequence number than older elements. Element also known as \"message\". Processing may include querying, filtering, and aggregating elements. The picture below illustrates the concepts:\nThese often go hand in hand with other publish-subscribe frameworks used for connecting applications and data stores. For example, Apache Kafka is a popular open source publish-subscribe framework that simplifies integrating data across multiple applications.\nBelow a few common streaming frameworks/ libraries are listed:\nApache Kafka Streams is a stream processing library for creating applications that ingest data from Kafka, process it and then publish the results back to Kafka as a new data source for other applications to consume.\nSamza is a distributed stream processing tool that allows users to build stateful applications.\nApache Storm supports real-time computation capabilities like online machine learning, reinforcement learning and continuous computation.\nDelta Lake supports stream processing and batch processing using a common architecture.\nIn micro-batch processing, we run batch processes on much smaller accumulations of data - typically less than a minute’s worth of data. This means data is available in near real-time. In practice, there is little difference between micro-batching and stream processing, and the terms would often be used interchangeably in data architecture descriptions and software platform descriptions. Microbatch processing is useful when we need very fresh data, but not necessarily real-time - meaning we can’t wait an hour or a day for a batch processing to run, but we also don’t need to know what happened in the last few seconds.\nBenefits are as follows:\nLoose coupling\nIf you write data to the database in the same schema as you use for reading, you have tight coupling between the part of the application doing the writing (the \"button\") and the part doing the reading (the \"screen\"). We know that loose coupling is a good design principle for software. By separating the form in which you write and read data, and by explicitly translating from one to the other, you get much looser coupling between different parts of your application.\nRead and write performance\nThe decades-old debate over normalization (faster writes) vs. denormalization (faster reads) exists only because of the assumption that writes and reads use the same schema. If you separate the two, you can have fast writes and fast reads.\nScalability\nEvent streams are great for scalability, because they are a simple abstraction (comparatively easy to parallelize and scale across multiple machines) and because they allow you to decompose your application into producers and consumers of streams (which can operate independently and can take advantage of more parallelism in hardware).\nFlexibility and agility\nRaw events are so simple and obvious that a \"schema migration\" doesn’t really make sense (you may just add a new field from time to time, but you don’t usually need to rewrite historic data into a new format). On the other hand, the ways in which you want to present data to users are much more complex, and may be continually changing. If you have an explicit translation process between the source of truth and the caches that you read from, you can experiment with new user interfaces by just building new caches using new logic, running the new system in parallel with the old one, gradually moving people over from the old system, and then discarding the old system (or reverting to the old system if the new one didn’t work). Such flexibility is incredibly liberating.\nError scenarios\nFinally, error scenarios are much easier to reason about if data is immutable. If something goes wrong in your system, you can always replay events in the same order, and reconstruct exactly what happened (especially important in finance, where auditability is crucial). If you deploy buggy code that writes bad data to a database, you can just re-run it after you fixed the bug, and thus correct the outputs. Those things are not possible if your database writes are destructive.\nStandard Problems\nStandard challenges are:\nYou have to build a system that can cost-effectively collect, prepare, and transmit data coming simultaneously from thousands of data sources.\nYou need to fine-tune the storage and compute resources so that data is batched and transmitted efficiently for maximum throughput and low latency.\nYou have to deploy and manage a fleet of servers to scale the system so you can handle the varying speeds of data you are going to throw at it.\nIoT devices are often produced by different manufacturers so the data can be quite different. This scenario of mixed devices and sensor data means the data schema can change unpredictably, potentially breaking data pipelines.\nTable of Contents\nStream Processing Solutions - Apache Kafka on Microsoft Azure\nStream Processing Introduction\nContext &amp; Problem\nStandard Problems\nStream Processing Platforms\nAzure\nProvisioning Solutions - Apache Kafka on Microsoft Azure\nEvent Hub\nAKS\nConfluent Cloud\nHDInsight\nComparison\nCredits\nStream Processing Platforms\nAzure\nOverview\nThis chapter lists major features/ concrete services for stream processing on the Azure platform. This architecture pattern builds on the general problem description for stream processing.\nIn Azure, all of the following data stores will meet the core requirements supporting real-time processing:\nAzure Stream Analytics\nHDInsight with Spark Streaming (Microbacth processing)/ Kafka Streaming (Stream processing)\nApache Spark (Microbacth processing) in Azure Databricks\nHDInsight with Storm (Stream processing)\nAzure Functions\nAzure App Service WebJobs\nApache Kafka streams API\nFor Kafka a separate page with guidelines exists. The Azure Docs provide tables with decision criteria for the following technology options: Azure Stream Analytics, HDInsight with Spark Streaming, Apache Spark in Azure Databricks, HDInsight with Storm, Azure Functions, Azure App Service WebJobs.\nProvisioning Solutions - Apache Kafka on Microsoft Azure\nEvent Hub\nAzure Event Hub[1] supports Apache Kafka producer and consumer APIs\n+ Simplest option\n- No compression\n- No Kafka streams\nAKS\nKafka on Kubernetes (AKS) using Operators[2]\nrack awareness to spread brokers across availability zones\nKubernetes taints and tolerations to run Kafka on dedicated nodes\nexpose Kafka outside Kubernetes using NodePort\nLoad balancer and Ingress, depending on your needs\neasily secured using TLS\n+ Full control\n+ dedicated cluster\n- Most complex option\n- potentially higher compute and storage cost\nConfluent Cloud\nConfluent Cloud[3] is now integrated in Azure Marketplace\nQuickStart\nYou need to sign up for an account with Confluent in addition to the Azure account\nObservability via\nConfluent Control Center[4]\nmonitor system health in predefined dashboards and to alert on triggers\nreports end-to-end stream monitoring\nmeasures how long messages take\ndetermines the source of any issues in your cluster\nConfluent Metric API, and e.g. Prometheus and Grafana\n+ Reliable managed service\n+ Confluent ecosystem: Kafka Connect, KSQL, Connectors (source &amp; sink both), Confluent Schema Registry, Replicator\n- No dedicated cluster (just multi-tenant cluster)\n- No BYOK for data encryption\nHDInsight\nAzure HDInsight[5] is a fully-managed cloud distribution of Hadoop Components. It supports many open-source frameworks like Apache Spark, Hive, Apache Storm, R Server, Apache HBase and of course Apache Kafka. It provides 99.99% SLA on Kafka uptime &amp; uses Azure Managed Disks as the backing store for Kafka brokers (providing up to 16TB of storage per broker).\nSecurity:\nVNet Integration (cluster can be totally isolated)\nIntegration with Azure AD, no Kafka ACLs, though\nDomain Joined VMs (services on VMs run seamlessly for authenticated users)\nBYOK &amp; Managed Identity\nMonitoring:\nIntegration with Azure Monitoring\n+ Fully managed\n+ Dedicated cluster\n- No autoscale\n- Dedicated cluster also means higher cost for compute and storage\nComparison\nThere is no “one size fits all” solution, of course.\nAgraj Mangal from Microsoft provides a nice comparison (see table below).\nIf you need a dedicated cluster, you should go with AKS or HDInsight.\nFor BYOK-requirements, go with anything but Confluent Cloud.\nBut, if you want the most Kafka features with the least amount of customer-management necessity, go with Confluent Cloud.\nFor simple Kafka-API-Integration without any management necessity, go with Event Hub.\nKafka on VM is only recommended as a first step, to try out Kafka.\nTable 1. Comparison\nCriteria\\Service\nKafka on VM\nEvent Hub\nKafka on AKS\nConfluent Cloud\nHDInsight\nPrimary Use Case\nEasy first step in Cloud\nEasy One Click Replacement for Kafka\nPreferred if already invested in K8s\nCompletely Managed 3rd Party; PAYG model\nEnterprise-grade security, BYOK, Azure-native\nHigh Availability\nManaged by Customer\nEase of Setup\nSimple for Single broker, gets unmanageeable for bigger clusters\nOperators make it relatively easy. K8s knowledge required\nCost\nDependent on the VM type you choose\nStandard Tier costs about $20/month/Throughput Unit\nCost of Operator + Infra Used for Broker &amp; Storage\nConsumption based pricing model - PAYG\nCost for 1 Cluster approx. $2.90/hour\nApache Kafka Compatability\nMost features work OOTB, some does not work\nUp to Kafka 2.1.0 supported\nScalability\nManaged by Customer\nOperators help with scaling\nManaged Service\nManaged by Customer\nCredits\nMain Source: Agraj Mangal, https://itnext.io/apache-kafka-in-azure-6985ccdce89f\n1. https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview\n2. e.g. https://strimzi.io/ or Confluent Operator\n3. https://docs.microsoft.com/en-us/azure/partner-solutions/apache-kafka-confluent-cloud/overview\n4. https://docs.confluent.io/platform/current/control-center/index.html#control-center\n5. https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-introduction\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2022-11-22 10:06:45 UTC\nif(!document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nanchors.options.visible = 'always';\n}\nanchors.add();\nvar internalUrls = [\"https://cginternal.devonfw.com/websitesnippets/\"];\n$(\".internal\").each(function(){\ninternalUrls.forEach((internalUrl, index) => {\n$.ajax({url: internalUrl + $(this).text() + \"/index.html\" })\n.then(r => {\nvar leftDiv = $('Internal');\nvar rightFirstDiv = $('This is only visible inside of the corporate network.');\nvar rightLastDiv = $('');\nrightLastDiv.html(r);\nvar rightDiv = $('');\nrightDiv.append(rightFirstDiv);\nrightDiv.append(rightLastDiv);\n$(this).html(leftDiv);\n$(this).append(rightDiv);\n$(this).addClass(\"internal-active\");\n$(this).removeClass(\"internal\");\n})\n});\n});\nlet cookieName = 'cookie_consent';\nlet expiryTime = new Date(2037, 11, 21);\nlet today = new Date();\nlet cookieConsentDeclinedDate = localStorage.getItem(\"cookie_consent_declined\");\nlet dateDiffHours = 0;\nlet cookieConsentHtml = `\nWe use cookies to perform analytics and enhance user experience. Do you accept to the use of cookies?\nDecline\nAccept\n`;\nconst parseCookie = str =>\nstr\n.split(';')\n.map(v => v.split('='))\n.reduce((acc, v) => {\nacc[decodeURIComponent(v[0].trim())] = decodeURIComponent(v[1].trim());\nreturn acc;\n}, {});\ndateDiffHours = cookieConsentDeclinedDate ? Math.abs(cookieConsentDeclinedDate - today) / 36e5 : 0;\nif (document.cookie.indexOf(cookieName) 24) {\n$(\"body\").append(cookieConsentHtml);\n}\n} else {\n$(\"body\").append(cookieConsentHtml);\n}\n} else {\ncookies = parseCookie(document.cookie);\nif (cookies[\"cookie_consent\"] == \"accept\") {\nconsentGranted();\n}\n}\n$(\"#accept-cookie-btn\").click(function() {\ndocument.cookie = `${cookieName}=accept;expires=${expiryTime};path=/`;\nlocalStorage.removeItem(\"cookie_consent_declined\");\nconsentGranted();\n$(\"#cookie-consent\").hide();\n});\n$(\"#decline-cookie-btn\").click(function() {\nlocalStorage.setItem(\"cookie_consent_declined\", today);\n$(\"#cookie-consent\").hide();\n});\n"}]