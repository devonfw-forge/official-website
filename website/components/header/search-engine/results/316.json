{"type":"doc","filename":"devonfw-guide_devon4net.wiki_packages.asciidoc.html","anchor":"devonfw-guide_devon4net.wiki_packages.asciidoc_devon4net.infrastructure.kafka","title":"Devon4Net.Infrastructure.Kafka","breadcrumbs":[".net","Packages","Packages"],"text":"Devon4Net.Infrastructure.Kafka\n\nApache Kafka is an open-source distributed event streaming platform. Event streaming is the practice of capturing a stream of events and store it for later being able to retrieve it for processing it in the desired form. It guarantees a continuous flow of data between components in a distributed system. You can think of it as a data bus where components of a system can publish some events and can subscribe to others, the following diagram shows perfectly how the system works:\n\n\n\n\n\nFigure 83. Kafka diagram\n\n\nIn the image you can see how an event is sent to the Kafka server. This Event is a record of an action that happened and typically contains a key, value, timestamp and some metadata.\n\n\nThis events are published by Producers, who are those client applications that write to Kafka; and readed and processed by Consumers, who are the clients subscribed to the different topics.\n\n\nTopics are the organization type of Kafka events, similar to a folder on a filesystem, being events the files in that folder. Unlike message queues, Kafka events are not deleted after being read. Instead you can choose how much time should Kafka keep track of the events.\n\n\nOther interesting concepts about Kafka are:\n\n\n\n\nPartitions: Topics are divided into partitions. When a new event is published to a topic, it is actually appended to one of the topic’s partitions. Events with the same event key are written to the same partition.\n\n\nReplication: To make your data fault-tolerant and highly-available, every topic can be replicated so that there are always multiple brokers that have a copy of the data just in case things go wrong.\n\n\n\n\nConfiguration\n\nThe component configuration can be done in appsettings.{environment}.json with the following section:\n\n\n\n\n\n\n\n\n\nEnableKafka: Boolean to activate the use of Apache Kafka.\n\n\nAdministration:\n\n\n\nAdminId: Admin Identifier\n\n\nServers: Host address and port number in the form of host:port.\n\n\n\n\n\nProducers: List of all kafka producers configuration.\n\n\n\nProducerId: Identifier of the producer in devon.\n\n\nServers: Host address and port number in the form of host:port.\n\n\nClientId: Identifier of the client in Kafka.\n\n\nTopic: Topics where the event will be delivered.\n\n\nMessageMaxBytes: Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic’s max.message.bytes limit (see Apache Kafka documentation).\n\n\nCompressionLevel: Compression level parameter for algorithm selected by configuration property compression.codec. Higher values will result in better compression at the cost of more CPU usage. Usable range is algorithm-dependent:\n\n[0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent\n\n\nDefault is -1.\n\n\n\nCompressionType: compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property compression.codec. Types are: None, Gzip, Snappy, Lz4, Zstd. Default is None.\n\n\nReceiveMessageMaxBytes: Maximum Kafka protocol response message size. Default is 100000000.\n\n\nEnableSslCertificateVerification: Enable OpenSSL’s builtin broker (server) certificate verification. Default is true.\n\n\nCancellationDelayMaxMs: The maximum time in milliseconds before a cancellation request is acted on. Low values may result in measurably higher CPU usage. Default is 100.\n\n\nAck:\n\n\n\n\n\n\n\nValue\nDescription\n\n\nNone - default\nBroker does not send any response/ack to client\n\n\nLeader\nThe leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers\n\n\nAll\nBroker will block until message is committed by all in sync replicas (ISRs). If there are less than min.insync.replicas (broker configuration) in the ISR set the produce request will fail\n\n\n\n\nDefault is None.\n\n\n\nDebug:\nA comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch\n\n\nBrokerAddressTtl: How long to cache the broker address resolving results in milliseconds.\n\n\nBatchNumMessages: Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead. This limit is applied after the first message has been added to the batch, regardless of the first message’s size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet size is also limited by batch.num.messages and message.max.bytes\n\n\nEnableIdempotence: When set to true, the producer will ensure that messages are successfully produced exactly once and in the original produce order. The following configuration properties are adjusted automatically (if not modified by the user) when idempotence is enabled: max.in.flight.requests.per.connection=5 (must be less than or equal to 5), retries=INT32_MAX (must be greater than 0), acks=all, queuing.strategy=fifo. Producer instantation will fail if user-supplied configuration is incompatible\n\n\nMaxInFlight: Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one. Default is 5.\n\n\nMessageSendMaxRetries: How many times to retry sending a failing Message. Default is 5.\n\n\nBatchSize: Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead. This limit is applied after the first message has been added to the batch, regardless of the first message’s size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet size is also limited by batch.num.messages and message.max.bytes. Default is 1000000.\n\n\n\n\n\nConsumers: List of consumers configurations.\n\n\n\nConsumerId: Identifier of the consumer for devon.\n\n\nServers: Host address and port number in the form of host:port.\n\n\nGroupId: Client group id string. All clients sharing the same group.id belong to the same group.\n\n\nTopics: Topics where the event will be read from.\n\n\nAutoCommit: Automatically and periodically commit offsets in the background. Note: setting this to false does not prevent the consumer from fetching previously committed start offsets. To circumvent this behaviour set specific start offsets per partition in the call to assign()\n\n\nStatisticsIntervalMs: librdkafka statistics emit interval. The application also needs to register a stats callback using rd_kafka_conf_set_stats_cb(). The granularity is 1000ms. A value of 0 disables statistics\n\n\nSessionTimeoutMs: Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance. Default is 0.\n\n\nAutoOffsetReset: Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error which is retrieved by consuming messages and checking 'message-&gt;err'\n\n\nEnablePartitionEof: Verify CRC32 of consumed messages, ensuring no on-the-wire or on-disk corruption to the messages occurred. This check comes at slightly increased CPU usage\n\n\nIsolationLevel: Controls how to read messages written transactionally: ReadCommitted - only return transactional messages which have been committed. ReadUncommitted - return all messages, even transactional messages which have been aborted.\n\n\nEnableSslCertificateVerification: Enable OpenSSL’s builtin broker (server) certificate verification. Default is true.\n\n\nDebug: A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch\n\n\n\n\n\n\n\n\nSetting up in Devon\n\nFor setting it up using the Devon4Net WebApi template just configure it in the appsettings.Development.json. You can do this by copying the previously showed configuration with your desired values.\n\n\n\n\n\n\n\n\nPlease refer to the \"How to use Kafka\" and \"Kafka template\" documentation to learn more about Kafka.\n\n\n\n\n\n\nSetting up in other projects\n\nFor setting it up in other projects install it running the following command in the Package Manager Console, or using the Package Manager in Visual Studio:\n\n\n\n\n\n\n\nThis will install all the packages the component needs to work properly. Now set the configuration in the appsettings.{enviroment}.json:\n\n\n\n\n\n\n\nNavigate to your Program.cs file and add the following lines to configure the component:\n\n\n\n\n\n\n\nAs you will be able to tell, the process is very similar to installing other components. Doing the previous actions will allow you to use the different handlers available with kafka. You can learn more\n\n\n\n\n\n\n\n\nPlease refer to the \"How to use Kafka\" and \"Kafka template\" documentation to learn more about Kafka.\n\n\n\n\n\n\n\n"}