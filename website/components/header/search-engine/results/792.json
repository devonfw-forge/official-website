{"type":"doc","filename":"devonfw-guide_mrchecker.wiki_master-mrchecker.asciidoc_test-framework-modules.html","anchor":"devonfw-guide_mrchecker.wiki_master-mrchecker.asciidoc_devops-test-module","title":"DevOps Test Module","breadcrumbs":["Tools","MrChecker - devonfw testing tool","Test Framework Modules"],"text":"DevOps Test Module\n\n\nDevOPS Test Module\n\nWhat does DevOps mean for us?\n\nDevOps consists of a mixture of three key components in a technical project:\n\n\n\n\nPeople’s skills and mindset\n\n\nProcesses\n\n\nTools\n\n\n\n\nUsing E2E MrChecker Test Framework it is possible to cover the majority of these areas.\n\n\n\nQA Team Goal\n\nFor QA engineers, it is essential to take care of the product code quality.\n\n\nTherefore, we have to understand, that a test case is also code which has to be validated against quality gates. As a result, we must test our developed test case like it is done during standard Software Delivery Life Cycle.\n\n\n\nWell rounded test case production process\n\n\n\nHow do we define top-notch test cases development process in E2E MrChecker Test Framework\n\n\n\n\n\n\n\n\n\n\nContinuous Integration (CI) and Continuous Delivery (CD)\n\n\n\nContinuous Integration (CI) - a procedure where quality gates validate test case creation process\n\n\nContinuous Delivery (CD) - a procedure where we include created test cases, validated against CI, as smoke/regression/security\n\n\n\n\n\n\n\n\n\n\nWhat should you receive from this DevOps module\n\n\n\n\n\n\n\nWhat will you gain with our DevOps module\n\nThe CI procedure has been divided into transparent modules. This solution makes configuration and maintenance very easy because everyone is able to manage versions and customize the configuration independently for each module. A separate security module ensures the protection of your credentials and assigned access roles regardless of changes in other modules.\n\n\n\n\n\n\n\nYour CI process will be matched to the current project. You can easily go back to the previous configuration, test a new one or move a selected one to other projects.\n\n\n\n\n\n\n\nDevOps module supports a delivery model in which executors are made available to the user as needed. It has such advantages as:\n\n\n\n\nSaving computing resources\n\n\nEliminating guessing on your infrastructure capacity needs\n\n\nNot spending time on running and maintaining additional executors\n== How to build this DevOps module\n\n\n\n\nOnce you have implemented the module, you can learn more about it here:\n\n\n\n\nDocker commands\n\n\n\n\n\n\nContinuous Integration\n\nEmbrace quality with Continuous Integration while you produce test case(s).\n\n\nOverview\n\nThere are two ways to set up your Continuous Integration environment:\n\n\n\n\nCreate a Jenkins instance from scratch (e.g. by using the Jenkins Docker image)\n\nUsing a clean Jenkins instance requires the installation of additional plugins. The plugins required and their versions can be found on this page.\n\n\n\nUse thre pre-configured custom Docker image provided by us\n\nNo more additional configuration is required (but optional) using this custom Docker image. Additionally, this Jenkins setup allows dynamical scaling across multiple machines and even cloud (AWS, Azure, Google Cloud etc.).\n\n\n\n\n\n\nJenkins Overview\n\nJenkins is an Open Source Continuous Integration Tool. It allows the user to create automated build jobs which will run remotely on so called Jenkins Slaves. A build job can be triggered by several events, for example on new pull request on specified repositories or timed (e.g. at midnight).\n\n\nJenking Configuration\n\nTests created by using the testing framework can easily be implemented on a Jenkins instance. The following chapter will describe such a job configuration. If you’re running your own Jenkins instance, you may have to install additional plugins listed on the page Jenkins Plugins for a trouble-free integration of your tests.\n\n\nInitial Configuration\n\nThe test job is configured as a so-called parameterized job. This means, after starting the job, parameters can be specified, which will then be used in the build process. In this case, branch and testname will be expected when starting the job. These parameters specify which branch in the code repository should be checked out (possibly feature branch) and the name of the test that should be executed.\n\n\n\n\n\n\n\n\nBuild Process Configuration\n\n\n\nThe first step inside the build process configuration is to get the author of the commit that was made. The mail will be extracted and gets stored in a file called build.properties. This way, the author can be notified if the build fails.\n\n\n\n\n\n\n\nNext up, Maven will be used to check if the code can be compiled, without running any tests.\n\n\n\n\n\n\nAfter making sure that the code can be compiled, the actual tests will be executed.\n\n\n\n\n\n\n\n\nFinally, reports will be generated.\n\n\n\n\n\n\n\n\n\n\nPost Build Configuration\n\n\n\nAt first, the results will be imported to the Allure System\n\n\n\n\n\n\n\nJUnit test results will be reported as well. Using this step, the test result trend graph will be displayed on the Jenkins job overview.\n\n\n\n\n\n\n\nFinally, an E-Mail will be sent to the previously extracted author of the commit.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the Pre-Configured Custom Docker Image\n\nIf you are starting a new Jenkins instance for your tests, we’d suggest using the pre-configured Docker image. This image already contains all the configurations and additional features.\n\n\nThe configurations are e.g. Plugins and Pre-Installed job setup samples. This way, you don’t have to set up the entire CI-Environment from the ground up.\n\n\nAdditional features from this docker image allow dynamic creation and deletion of Jenkins slaves, by creating Docker containers. Also, Cloud Solutions can be implemented to allow wide-spread load balancing.\n\n\n\n\nContinuous Delivery\n\nInclude quality with Continuous Delivery during product release.\n\n\n\n\n\n\n\nOverview\n\nCD from Jenkins point of view does not change a lot from Continuous Integration one.\n\n\n\nJenkins Overview\n\nUse the same Jenkins settings for Jenkins CD setup as for CI, please. link. The only difference is:\n\n\n\n\nWhat type of test you will execute. Before, we have been choosing test case(s), now we will choose test suite(s)\n\n\nWho will trigger the given Smoke/Integration/Performance job\n\n\nWhat is the name of official branch. This branch ought always to use be used in every CD execution. It will be either master or develop.\n\n\n\n\nJenkins for Smoke Tests\n\nIn the $TESTNAME variable, where we input the test name( link ), please input the name of a test suite assembled together of tests tagged as smoke tests -( link ) thus running all the smoke tests.\n\n\n\nJenkins for Performance Tests\n\nUnder construction - added when WebAPI module is included.\n\n\n\n\n\nPipeline structure\n\nPipeline configuration:\n\nThe default interaction with Jenkins required manual jobs. This keeps configuration of a job in Jenkins separate from source code. With Pipeline plugin users can implement a pipeline procedure in Jenkinsfile and store it in repository with other code. This approach is used in Mr Checker framework. More info: https://jenkins.io/solutions/pipeline/\n\n\nOur CI &amp; CD processes are divided into a few separate files: Jenkins_node.groovy is the file to manage all processes. It defines all operations executed on a Jenkins node, so all code in this file is closed in node closure. Workflow in Jenkinsfile:\n\n\n\n\nRead all parameters from a Jenkins job\n\n\nExecute stage to prepare the environment\n\n\nExecute git pull command\n\n\nSet Jenkins job description\n\n\nExecute compilation of the project in a special prepared docker container\n\n\nExecute unit tests\n\n\nExecute integration tests\n\n\nDeploy artifacts to a local repository\n\n\nDeploy artifacts to an external repository (nexus/arifactory)\n\n\n\n\nNot all the steps must be present in the Jenkins files. This should be configured for particular job requirements.\n\n\n\nDescription of stages:\n\nStage “Prepare environment”\n\nFirst thing to do in this stage is overwriting properties loaded from Jenkins job. It is defined in “overrideProperties” function. The next function, “setJenkinsJobVariables” defines environment variables such as :\n\n\n\n\nJOB_NAME_UPSTREAM\n\n\nBUILD_DISPLAY_NAME_UPSTREAM\n\n\nBUILD_URL_UPSTREAM\n\n\nGIT_CREDENTIALS\n\n\nJENKINS_CREDENTIALS\n\n\n\n\nThe last function in the stage – “setWorkspace”  -creates an environment variable with path to local workspace. This is required beacuse when using pipeline plugin, Jenkins does not create the WORKSPACE env variables.\n\n\n\nStage \"Git pull\"\n\nIt pulls sources from the repository and loads “git pull” file which contains additional methods:\n\n\n\n\nsetGitAuthor – setting properties about git author to the file “build.properties” and loading created file\n\n\ntryMergeWithBranch – checking if actual branch can be merged with default main branch\n\n\n\n\n\nStage “Build compile”\n\nVerify with maven that code builds without errors\n\n\n\nStage “Unit test”\n\nExecute unit tests with mvn surefire test and publish reports in junit and allure format\n\n\n\nStage “Integration test”\n\nExecute integration tests with mvn surefire test and publish reports in junit and allure format\n\n\n[[devonfw-guide_mrchecker.wiki_who-is-mrchecker_test-framework-modules_devops-test-module-pipeline-structure.asciidoc_stage-deploy-–-local-repo]]\n=== Stage “Deploy – local repo”\n\n\nArchive artifacts as a jar file in the local repository\n\n\n[[devonfw-guide_mrchecker.wiki_who-is-mrchecker_test-framework-modules_devops-test-module-pipeline-structure.asciidoc_stage-deploy-–-nexu-repo]]\n=== Stage ”Deploy – nexu repo”\n\n\nDeploy to the external repository with maven release deploy command with credentials stored in Jenkins machine. Additional files:\n\n\n\n\nmailSender.groovy – contains methods for sending mail with generated content\n\n\nstashNotification.groovy – send job status for bitbucket by a curl command\n\n\nutils.groovy - contains additional functions to load properties, files and generate additional data\n\n\n\n\n\n\n\nSelenium Grid\n\nWhat is Selenium Grid\n\nSelenium Grid allows running web/mobile browsers test cases to fulfill basic factors, such as:\n\n\n\n\nIndependent infrastructure, similar to end-users'\n\n\nScalable infrastructure (\\~50 simultaneous sessions at once)\n\n\nHuge variety of web browsers (from mobile to desktop)\n\n\nContinuous Integration and Continuous Delivery process\n\n\nSupporting multi-type programming languages (java, javascript, python, …​).\n\n\n\n\n\n\n\n\n\nOn a daily basis, a test automation engineer uses their local environments for test case execution/development. However, a created browser test case has to be able to run on any  infrastructure. Selenium Grid enables this portability.\n\n\n\nSelenium Grid Structure\n\n\n\n\n\n\nFull documentation of Selenium Grid can be found here and here.\n\n\n'Vanilla flavour' Selenium Grid is based on two, not very complicated ingredients:\n\n\n\n\nSelenium Hub - as one machine, accepting connections to grid from test cases executors. It also plays a managerial role in connection to/from Selenium Nodes\n\n\nSelenium Node - from one to many machines, where on each machine a browser used during test case execution is installed.\n\n\n\n\n\nHow to setup\n\nThere are two options of Selenium Grid setup:\n\n\n\n\nClassic, static solution - link\n\n\nCloud, scalable solution - link\n\n\n\n\nAdvantages and disadvantages of both solutions:\n\n\n\n\n\n\n\n\nHow to use Selenium Grid with E2E Mr Checker Test Frameworks\n\nRun the following command either in Eclipse or in Jenkins:\n\n\n\n\n\n\n\nAs a result of this command:\n\n\n\n\n-Dtest=com.capgemini.ntc.selenium.features.samples.resolutions.ResolutionTest - name of test case to execute\n\n\n-DseleniumGrid=\"http://10.40.232.61:4444/wd/hub\" - IP address of Selenium Hub\n\n\n-Dos=LINUX - what operating system must be assumed during test case execution\n\n\n-Dbrowser=chrome - what type of browser will be used during test case execution\n\n\n\n\n\n\n\n\n\n\n\nList of Jenkins Plugins\n\n\n\n\n\n\n\nPlugin Name\nVersion\n\n\n\n\nblueocean-github-pipeline\n1.1.4\n\n\nblueocean-display-url\n2.0\n\n\nblueocean\n1.1.4\n\n\nworkflow-support\n2.14\n\n\nworkflow-api\n2.18\n\n\nplain-credentials\n1.4\n\n\npipeline-stage-tags-metadata\n1.1.8\n\n\ncredentials-binding\n1.12\n\n\ngit\n3.5.1\n\n\nmaven-plugin\n2.17\n\n\nworkflow-durable-task-step\n2.12\n\n\njob-dsl\n1.64\n\n\ngit-server\n1.7\n\n\nwindows-slaves\n1.3.1\n\n\ngithub\n1.27.0\n\n\nblueocean-personalization\n1.1.4\n\n\njackson2-api\n2.7.3\n\n\nmomentjs\n1.1.1\n\n\nworkflow-basic-steps\n2.6\n\n\nworkflow-aggregator\n2.5\n\n\nblueocean-rest\n1.1.4\n\n\ngradle\n1.27.1\n\n\npipeline-maven\n3.0.0\n\n\nblueocean-pipeline-editor\n0.2.0\n\n\ndurable-task\n1.14\n\n\nscm-api\n2.2.2\n\n\npipeline-model-api\n1.1.8\n\n\nconfig-file-provider\n2.16.3\n\n\ngithub-api\n1.85.1\n\n\npam-auth\n1.3\n\n\nworkflow-cps-global-lib\n2.8\n\n\ngithub-organization-folder\n1.6\n\n\nworkflow-job\n2.12.1\n\n\nvariant\n1.1\n\n\ngit-client\n2.5.0\n\n\nsse-gateway\n1.15\n\n\nscript-security\n1.29.1\n\n\ntoken-macro\n2.1\n\n\njquery-detached\n1.2.1\n\n\nblueocean-web\n1.1.4\n\n\ntimestamper\n1.8.8\n\n\ngreenballs\n1.15\n\n\nhandlebars\n1.1.1\n\n\nblueocean-jwt\n1.1.4\n\n\npipeline-stage-view\n2.8\n\n\nblueocean-i18n\n1.1.4\n\n\nblueocean-git-pipeline\n1.1.4\n\n\nace-editor\n1.1\n\n\npipeline-stage-step\n2.2\n\n\nemail-ext\n2.58\n\n\nenvinject-api\n1.2\n\n\nrole-strategy\n2.5.1\n\n\nstructs\n1.9\n\n\nlocale\n1.2\n\n\ndocker-workflow\n1.13\n\n\nssh-credentials\n1.13\n\n\nblueocean-pipeline-scm-api\n1.1.4\n\n\nmetrics\n3.1.2.10\n\n\nexternal-monitor-job\n1.7\n\n\njunit\n1.21\n\n\ngithub-branch-source\n2.0.6\n\n\nblueocean-config\n1.1.4\n\n\ncucumber-reports\n3.8.0\n\n\npipeline-model-declarative-agent\n1.1.1\n\n\nblueocean-dashboard\n1.1.4\n\n\nsubversion\n2.9\n\n\nblueocean-autofavorite\n1.0.0\n\n\npipeline-rest-api\n2.8\n\n\npipeline-input-step\n2.7\n\n\nmatrix-project\n1.11\n\n\npipeline-github-lib\n1.0\n\n\nworkflow-multibranch\n2.16\n\n\ndocker-plugin\n0.16.2\n\n\nresource-disposer\n0.6\n\n\nicon-shim\n2.0.3\n\n\nworkflow-step-api\n2.12\n\n\nblueocean-events\n1.1.4\n\n\nworkflow-scm-step\n2.6\n\n\ndisplay-url-api\n2.0\n\n\nfavorite\n2.3.0\n\n\nbuild-timeout\n1.18\n\n\nmapdb-api\n1.0.9.0\n\n\npipeline-build-step\n2.5.1\n\n\nantisamy-markup-formatter\n1.5\n\n\njavadoc\n1.4\n\n\nblueocean-commons\n1.1.4\n\n\ncloudbees-folder\n6.1.2\n\n\nssh-slaves\n1.20\n\n\npubsub-light\n1.10\n\n\npipeline-graph-analysis\n1.4\n\n\nallure-jenkins-plugin\n2.23\n\n\nmailer\n1.20\n\n\nws-cleanup\n0.33\n\n\nauthentication-tokens\n1.3\n\n\nblueocean-pipeline-api-impl\n1.1.4\n\n\nldap\n1.16\n\n\ndocker-commons\n1.8\n\n\nbranch-api\n2.0.10\n\n\nworkflow-cps\n2.36.1\n\n\npipeline-model-definition\n1.1.8\n\n\nblueocean-rest-impl\n1.1.4\n\n\nant\n1.7\n\n\ncredentials\n2.1.14\n\n\nmatrix-auth\n1.7\n\n\npipeline-model-extensions\n1.1.8\n\n\npipeline-milestone-step\n1.3.1\n\n\njclouds-jenkins\n2.14\n\n\nbouncycastle-api\n2.16.1\n\n\n\n\n\nWhat is Docker\n\nDocker is an open source software platform to create, deploy and manage virtualized application containers on a common operating system (OS), with an ecosystem of allied tools.\n\n\n\nWhere do we use Docker\n\nDevOps module consists of Docker images\n\n\n\n\nJenkins image\n\n\nJenkins job image\n\n\nJenkins management image\n\n\nSecurity image\n\n\n\n\nin addition, each new node is also based on Docker\n\n\n\nExploring basic Docker options\n\nLet’s show some of the most important commands that are needed when working with our DevOps module based on the Docker platform. Each command given below should be preceded by a sudo call by default. If you don’t want to use sudo command create a Unix group called docker and add a user to it.\n\n\n\n\n\n\n\nBuild an image from a Dockerfile\n\n\n\n\n\n\n\nContainer start\n\n\n\n\n\n\n\nRemove one or more containers\n\n\n\n\n\n\n\nList containers\n\n\n\n\n\n\n\nPull an image or a repository from a registry\n\n\n\n\n\n\n\nPush the image or a repository to a registry\n\nPushing new image takes place in two steps. First save the image by adding container ID to the commit command and next use push:\n\n\n\n\n\n\n\n\nReturn information on Docker object\n\n\n\n\n\n\n\nList images\n\n\n\n\n\n\n\nRemove one or more images\n\n\n\n\n\n\n\nRun a command in a running container\n\n\n\n\n\n\n\n\nAdvanced commands\n\nRemove dangling images\n\n\n\n\n\n\n\nRemove all images\n\n\n\n\n\n\n\nRemoving images according to a pattern\n\n\n\n\n\n\n\nRemove all exited containers\n\n\n\n\n\n\n\nRemove all stopped containers\n\n\n\n\n\n\n\nRemove containers according to a pattern\n\n\n\n\n\n\n\nRemove dangling volumes\n\n\n\n\n\n\n\n\n\n\n"}